{"cells":[{"cell_type":"markdown","source":["# 1. ETL"],"metadata":{}},{"cell_type":"code","source":["# Specify path to downloaded log file\nimport sys\nimport os\n\nlog_file_path = 'dbfs:/' + os.path.join('databricks-datasets', 'cs100', 'lab2', 'data-001', 'apache.access.log.PROJECT')\nbase_df = sqlContext.read.text(log_file_path)\nbase_df.printSchema()\nbase_df.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["| field         | meaning                                                                |\n| ------------- | ---------------------------------------------------------------------- |\n| _remotehost_  | Remote hostname (or IP number if DNS hostname is not available).       |\n| _rfc931_      | The remote logname of the user. We don't really care about this field. |\n| _authuser_    | The username of the remote user, as authenticated by the HTTP server.  |\n| _[date]_      | The date and time of the request.                                      |\n| _\"request\"_   | The request, exactly as it came from the browser or client.            |\n| _status_      | The HTTP status code the server sent back to the client.               |\n| _bytes_       | The number of bytes (`Content-Length`) transferred to the client.      |"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import split, regexp_extract\nsplit_df = base_df.select(regexp_extract('value', r'^([^\\s]+\\s)', 1).alias('host'),\n                          regexp_extract('value', r'^.*\\[(\\d\\d/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]', 1).alias('timestamp'),\n                          regexp_extract('value', r'^.*\"\\w+\\s+([^\\s]+)\\s+HTTP.*\"', 1).alias('path'),\n                          regexp_extract('value', r'^.*\"\\s+([^\\s]+)', 1).cast('integer').alias('status'),\n                          regexp_extract('value', r'^.*\\s+(\\d+)$', 1).cast('integer').alias('content_size'))\nsplit_df.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# are there any null values\n\nbad_rows_df = split_df.filter(split_df['host'].isNull() |\n                              split_df['timestamp'].isNull() |\n                              split_df['path'].isNull() |\n                              split_df['status'].isNull() |\n                             split_df['content_size'].isNull())\n\nprint \"number of rows with some null values\", bad_rows_df.count()\nprint \"this is a bad sign!!\"\n\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# let's find the null values:\nnulls = {}\nfor c in split_df.columns:\n  nulls[c] = split_df.filter(split_df[c].isNull()).count()  \nnulls\nprint \"It looks like it is coming from content_size column\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["bad_content_size_df = base_df.filter(~base_df['value'].rlike('\\d+$'))\nbad_content_size_df.show(truncate=False)\nbad_content_size_df.count()\n# the count matches with the null values '-' indicates zero size "],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["cleaned_df = split_df.na.fill(0)\n# let's find the null values after replacing them with 0:\nnulls = {}\nfor c in split_df.columns:\n  nulls[c] = split_df.filter(split_df[c].isNull()).count()  \nprint nulls\nprint \"Horayyy. No null values\""],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["month_map = {\n  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n  'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n}\n\ndef parse_clf_time(s):\n    # We're ignoring time zone here.\n    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n      int(s[7:11]),\n      month_map[s[3:6]],\n      int(s[0:2]),\n      int(s[12:14]),\n      int(s[15:17]),\n      int(s[18:20])\n    )\nu_parse_time = udf(parse_clf_time)\nlogs_df = cleaned_df.select('*', u_parse_time(cleaned_df['timestamp']).cast('timestamp').alias('time')).drop('timestamp')\ntotal_log_entries = logs_df.count()\n\nprint \"total log entires\" , total_log_entries\nlogs_df.show(10,False)\nlogs_df.cache()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["# 2. Analysis"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as sql\ncontent_size_summary_df = logs_df.describe(['content_size'])\nlogs_df.agg(sql.min(logs_df.content_size),sql.avg(logs_df.content_size),sql.max(logs_df.content_size)).show()\n\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql import functions\nstatuses = logs_df.groupBy('status').count().orderBy('count', ascending = False)\n\nlog_status = statuses.withColumn('log(count)', functions.log(statuses['count']))\ndisplay(log_status)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["logs_df.groupBy('host').count().orderBy('count', ascending= False).show(5,truncate = False)\nlogs_df.groupBy('path').count().orderBy('count', ascending= False).show(5,truncate = False)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.functions import dayofmonth\nhost_day= logs_df.select(dayofmonth('time').alias('day'),'host').distinct()\ndaily_hosts_df = host_day.groupBy('day').count().orderBy('count',ascending=False)\ndaily_hosts_df.show(30)\n\nfrom spark_notebook_helpers import prepareSubplot, np, plt, cm\ndays = []\nunique_hosts = []\nfor r in daily_hosts_df.orderBy('day').collect():\n  days.append(r[0])\n  unique_hosts.append(r[1])\n  \nfig, ax = prepareSubplot(np.arange(0, 30, 5), np.arange(0, 5000, 1000))\ncolorMap = 'Dark2'\ncmap = cm.get_cmap(colorMap)\nplt.plot(days, unique_hosts, color=cmap(0), linewidth=3)\nplt.axis([0, max(days), 0, max(unique_hosts)+500])\nplt.xlabel('Day')\nplt.ylabel('Hosts')\nplt.axhline(linewidth=3, color='#999999')\nplt.axvline(linewidth=2, color='#999999')\ndisplay(fig)\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["host_day_count  = logs_df.select(dayofmonth('time').alias('day'),'host').groupBy('host','day').count()\navg_daily_req_per_host_df = host_day_count.groupBy('day').avg('count').orderBy('day')\navg_daily_req_per_host_df.show()\ndisplay(avg_daily_req_per_host_df)\nprint 'No significant findings here'"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["print 'top 404 paths'\nsplit_df.filter(split_df.status == '404').groupBy('path').count().orderBy('count', ascending = False).show(5,truncate = False)\nprint 'top 404 hosts'\nsplit_df.filter(split_df.status == '404').groupBy('host').count().orderBy('count', ascending = False).show(5,truncate = False)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.functions import hour\n\nperhour404 = logs_df.withColumn('hour',hour('time').cast('integer')).filter(split_df.status == '404').groupBy('hour').count().orderBy('hour')\ndisplay(perhour404)\n\n# perday404 = logs_df.withColumn('day',dayofmonth('time').cast('integer')).filter(split_df.status == '404').groupBy('day').count().orderBy('day')\n#perday404.collect()\n# days = perday404.rdd.map(lambda r:r['day']).collect()\n# errors = perday404.rdd.map(lambda r:r['count']).collect()\n# fig, ax = prepareSubplot(np.arange(0, 20, 5), np.arange(0, 600, 100))\n# colorMap = 'rainbow'\n# cmap = cm.get_cmap(colorMap)\n# plt.plot(days, errors, color=cmap(0), linewidth=3)\n# plt.axis([0, max(days), 0, max(errors)])\n# plt.xlabel('Day')\n# plt.ylabel('404 Errors')\n# plt.axhline(linewidth=3, color='#999999')\n# plt.axvline(linewidth=2, color='#999999')\n# display(fig)"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"Apache-Log-Analysis","notebookId":3554922149065530},"nbformat":4,"nbformat_minor":0}
