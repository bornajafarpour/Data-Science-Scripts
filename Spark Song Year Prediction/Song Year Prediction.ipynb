{"cells":[{"cell_type":"markdown","source":["# Predicting Songs' release years using Linear Regression\nWe use a subset of [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD). Our goal is to train a linear regression model to predict the release year of a song given a set of audio features."],"metadata":{}},{"cell_type":"markdown","source":["# 1. ETL"],"metadata":{}},{"cell_type":"code","source":["# load testing library\nfrom databricks_test_helper import Test\nimport os.path\nfile_name = os.path.join('databricks-datasets', 'cs190', 'data-001', 'millionsong.txt')\n\nraw_data_df = sqlContext.read.load(file_name, 'text')\nprint raw_data_df.count()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import lit\nfrom pyspark.mllib.regression import LabeledPoint\nparsedDF = raw_data_df.rdd.map(lambda x: x[0].split(','))\nlp_df = parsedDF.map(lambda x: LabeledPoint(x[0],x[1:])).toDF()\nlp_df.show(5)\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import matplotlib.pyplot as plt\ndef prepare_plot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n                 gridWidth=1.0):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n        axis.set_ticks_position('none')\n        axis.set_ticks(ticks)\n        axis.label.set_color('#999999')\n        if hideLabels: axis.set_ticklabels([])\n    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n    return fig, ax\n  \nimport numpy as np\nminyear = lp_df.groupBy().min('label').collect()[0][0]\nprint \"Earliest year\", minyear\nlp_df_year_adjusted = lp_df.withColumn('shifted_year',lp_df.label-lit(minyear)).drop('label').withColumnRenamed(\"shifted_year\",'label')\nprint lp_df_year_adjusted.take(5)\n\n\nnew_data = (lp_df_year_adjusted\n             .rdd\n             .map(lambda lp: (lp.label, 1))\n             .reduceByKey(lambda x, y: x + y)\n             .collect())\nx, y = zip(*new_data)\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(0, 120, 20), np.arange(0, 120, 20))\nplt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\nax.set_xlabel('Year (shifted)'), ax.set_ylabel('Count')\ndisplay(fig)\npass"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\ndata_values = (lp_df_year_adjusted\n               .rdd\n               .map(lambda lp: lp.features.toArray())\n               .takeSample(False, 50, 47))\n\n# We can uncomment the line below to see randomly selected features.  \n# data_values = (parsedPointsDF\n#                .rdd\n#                .map(lambda lp: lp.features.toArray())\n#                .takeSample(False, 50))\n\n# generate layout and plot\nfig, ax = prepare_plot(np.arange(.5, 11, 1), np.arange(.5, 49, 1), figsize=(8,7), hideLabels=True,\n                       gridColor='#eeeeee', gridWidth=1.1)\nimage = plt.imshow(data_values,interpolation='nearest', aspect='auto', cmap=cm.Greys)\nfor x, y, s in zip(np.arange(-.125, 12, 1), np.repeat(-.75, 12), [str(x) for x in range(12)]):\n    plt.text(x, y, s, color='#999999', size='10')\nplt.text(4.7, -3, 'Feature', color='#999999', size='11'), ax.set_ylabel('Observation')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# 2. Bulding a linear regression model from scratch"],"metadata":{}},{"cell_type":"code","source":["seed = 42\nweights = [.8, .1, .1]\ntrain,validation,test = lp_df_year_adjusted.randomSplit(weights,seed)\ntrain.cache()\nvalidation.cache()\ntest.cache()\n\n# print n_train, n_val, n_test, n_train + n_val + n_test\n\nprint \"train, test, validation\",train.count(),validation.count(), test.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import sqrt, sum, count\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ndef calc_RMSE(pl_df):\n  return pl_df.select(sqrt(sum((pl_df.prediction - pl_df.label)**2)/count(pl_df.label))).collect()[0][0]\n\n# spark alternative:\nevaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\npreds_and_labels = [(1., 3.), (2., 1.), (2., 2.)]\npreds_and_labels_df = sqlContext.createDataFrame(preds_and_labels, [\"prediction\", \"label\"])\nprint calc_RMSE(preds_and_labels_df)\nprint evaluator.evaluate(preds_and_labels_df)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\naverage_train_year = train.groupBy().avg('label').collect()[0][0]\ntrain_lp = train.select(train.label).withColumn('prediction',lit(average_train_year))\ntest_lp = test.select(test.label).withColumn('prediction',lit(average_train_year))\nvalidation_lp = validation.select(validation.label).withColumn('prediction',lit(average_train_year))\nprint 'Baseline Train RMSE = {0:.3f}'.format(evaluator.evaluate(train_lp))\nprint 'Baseline Validation RMSE = {0:.3f}'.format(calc_RMSE(test_lp))\nprint 'Baseline Test RMSE = {0:.3f}'.format(calc_RMSE(validation_lp))\n\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.mllib.linalg import DenseVector\n\ndef calc_delta(x, w,b):\n  e = (w.dot(x.features)+b-x.label)\n  return (e*x.features,e)\n\ndef predict_rdd(x,w,b):\n  return float(w.dot(x.features)+b),x.label\n\ndef lr_gd(train_data, num_iters):\n  w = np.zeros(len(train_data.first().features))\n  b=0\n  n = train_data.count()\n  error = []\n  alpha = 1.0\n  for i in range(num_iters):\n    pred_label = train_data.rdd.map(lambda x:predict_rdd(x,w,b)) # make prediction\n    pred_label_df = sqlContext.createDataFrame(pred_label, [\"prediction\", \"label\"])\n    error.append(calc_RMSE(pred_label_df))\n    #gradient_w,gradient_b, = train_data.rdd.map(lambda x:calc_delta(x,w,b)).reduce(lambda x,y:x+y)\n    gradient_w,gradient_b, = train_data.rdd.map(lambda x:calc_delta(x,w,b)).reduce(lambda (a,b),(c,d):(a+c,b+d))\n    alpha_i = alpha / (n * np.sqrt(i+1))\n    w -= alpha_i*gradient_w\n    b -= alpha_i*gradient_b\n  return w,b, error    \n\n\nnum_iters = 50\nweights_LR0,b50,e50 = lr_gd(train,num_iters)\n\n#Predicting using the trained model\npreds_and_labels = (validation.rdd.map(lambda lp: predict_rdd(lp,weights_LR0,b50)))\npreds_and_labels_df = sqlContext.createDataFrame(preds_and_labels, [\"prediction\", \"label\"])\nrmse_val_LR0 = calc_RMSE(preds_and_labels_df)\nprint \"Validation Error ->\" , rmse_val_LR0\n\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from matplotlib.colors import ListedColormap, Normalize\nfrom matplotlib.cm import get_cmap\ncmap = get_cmap('YlOrRd')\n\nnorm = Normalize()\nclrs = cmap(np.asarray(norm(e50[6:])))[:,0:3]\n\nfig, ax = prepare_plot(np.arange(0, 60, 10), np.arange(17, 22, 1))\nax.set_ylim(17.8, 21.2)\nplt.scatter(range(0, num_iters-6), e50[6:], s=14**2, c=clrs, edgecolors='#888888', alpha=0.75)\nax.set_xticklabels(map(str, range(6, 66, 10)))\nax.set_xlabel('Iteration'), ax.set_ylabel(r'Training Error')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["# 3. Spark MLlib LinearRegression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n# Values to use when training the linear regression model\n\nnum_iters = 500  # iterations\nreg = 1e-1  # regParam\nalpha = .2  # elasticNetParam\nuse_intercept = True  # intercept\nlin_reg = LinearRegression(maxIter = num_iters , elasticNetParam = alpha ,regParam = reg, fitIntercept = use_intercept)\nfirst_model = lin_reg.fit(train)\n\n#coeffsLR1 stores the model coefficients; interceptLR1 stores the model intercept\ncoeffs_LR1 =first_model.coefficients\nintercept_LR1 = first_model.intercept\nprint coeffs_LR1, intercept_LR1\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#let's do some predictions\nval_pred_df = first_model.transform(validation )\nrmse_val_LR1 = calc_RMSE(val_pred_df)\n\nprint ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 (My implementation) = {1:.3f}' +\n       '\\n\\tLR1 (PySpark) = {2:.3f}').format(calc_RMSE(validation_lp), rmse_val_LR0, rmse_val_LR1)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nreg_params = [1.0, 2.0, 4.0, 8.0, 16.0, 32.0]\nalpha_params = [0.0, .1, .2, .4, .8, 1.0]\nlr = LinearRegression(maxIter=50,fitIntercept = True)\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.elasticNetParam, alpha_params) \\\n    .addGrid(lr.regParam, reg_params) \\\n    .build()\n\ncrossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=2) \n\ncvModel = crossval.fit(train)\nrmse_val_LR_grid = evaluator.evaluate(cvModel.transform(validation))\nprint \"Validation error of the best model ->\",rmse_val_LR_grid\n\nprint ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 (My implementation) = {1:.3f}' +\n       '\\n\\tLR1 (PySpark) = {2:.3f}'+'\\n\\tLRGrid (Hyper Parameter Optimized) = {2:.3f}').format(calc_RMSE(validation_lp), rmse_val_LR0, rmse_val_LR1,rmse_val_LR_grid)\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["def addf(x):# create 2way interactions in addition to existing variables\n  newf = [a*b for a in x['features'] for b in x['features']]\n  return LabeledPoint(x['label'],np.append(x['features'],newf))\n\ntrain2way = train.map(lambda lp:addf(lp)).toDF()\nvalidation2way = validation.map(lambda lp:addf(lp)).toDF()\n\nnum_iters = 500\nreg = 1e-10\nalpha = .2\nuse_intercept = True\n\nlin_reg = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha, fitIntercept=use_intercept)\nmodel_interact = lin_reg.fit(train2way)\nval_2way_predict = model_interact.transform(validation2way)\nrmse_val_2way = calc_RMSE(val_2way_predict)\n\nprint ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 (my implementation) = {1:.3f}\\n\\tLR1 (PySpark) = {2:.3f}\\n\\tLRGrid (Hyper Parameter Optimized) = ' + '{3:.3f}\\n\\tLRInteract (2way interactions) = {4:.3f}').format(evaluator.evaluate(train_lp), rmse_val_LR0, rmse_val_LR1,\n                                                 rmse_val_LR_grid, rmse_val_2way)\nprint \"\\nAdding two way interactions help\""],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import PolynomialExpansion\nnum_iters = 500\nreg = 1e-10\nalpha = .2\nuse_intercept = True\n\n\npe2_pipe = PolynomialExpansion(degree=2, inputCol=\"features\", outputCol=\"polyFeatures\")\nlr_pipe = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha,\n                                     fitIntercept=use_intercept, featuresCol='polyFeatures')\n#defining the pipeline and its \npl = Pipeline(stages=[pe2_pipe,lr_pipe])\n\npl_model = pl.fit(train)\n\nprint \"Validation set RMSE using the pipeline ->\", evaluator.evaluate(pl_model.transform(validation))\n\n"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"Song Year Prediction","notebookId":3446417719326252},"nbformat":4,"nbformat_minor":0}
