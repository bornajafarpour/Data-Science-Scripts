{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own BackPropagation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#let's define some basic functions. Even though I am not using anything besides sigmoid function\n",
    "#, I have defined the softmax and the derivative \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(np.zeros(x.shape[0]),x)\n",
    "\n",
    "def identity_derivative(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def sigmoid_derivative(sigmoid_x): # we calculate the derivative based on the sigmoid function value\n",
    "    return sigmoid_x*(1-sigmoid_x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1-x**2\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid:\n",
    "        return sigmoid_derivative\n",
    "    elif f == np.tanh:\n",
    "        return tanh_derivative\n",
    "    elif f == identity:\n",
    "        return identity_derivative\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def local_max_indices(x,pool_d): #if is not devidable wihch can happen as a result of conv, we need to do something about it\n",
    "    \"\"\"Return maximum in groups of pool_dxpool_d for a N,h,w image\"\"\"\n",
    "    N,h,w = x.shape\n",
    "    x = x.reshape(N,h/pool_d,pool_d,w/pool_d,pool_d).swapaxes(2,3).reshape(N,h/pool_d,w/pool_d,pool_d*pool_d)\n",
    "    return np.argmax(x,axis=3)\n",
    "\n",
    "def global_max_indices(x,pool_d):\n",
    "    N = x.shape[0]\n",
    "    image_d= x.shape[1]\n",
    "    ip_ratio = image_d / pool_d\n",
    "    lmi = local_max_indices(x,pool_d)\n",
    "    max_local_x,max_local_y = np.unravel_index(lmi.flat,dims=(pool_d,pool_d))\n",
    "    max_y =  max_local_y + np.tile(np.tile(range(ip_ratio),ip_ratio)*pool_d,N)\n",
    "    max_x =  max_local_x + np.tile(np.repeat(np.arange(ip_ratio), ip_ratio)*pool_d,N)\n",
    "    Ns = np.repeat(np.arange(N),ip_ratio**2)\n",
    "    return np.vstack([Ns,max_x,max_y]).T\n",
    "\n",
    "def maxpool(x,pool_d):\n",
    "    N= x.shape[0]\n",
    "    image_d = x.shape[1]\n",
    "    crop_length = image_d%pool_d\n",
    "    x = x[:,:image_d-crop_length,:image_d-crop_length]\n",
    "    gmi = global_max_indices(x,pool_d)\n",
    "    maxes =  x[gmi[:,0],gmi[:,1],gmi[:,2]].reshape(N,image_d/pool_d,image_d/pool_d)\n",
    "    return crop_length,gmi,maxes\n",
    "\n",
    "def up_sample(gmi,values,image_d,pool_d,crop_length):\n",
    "    N =values.shape[0] \n",
    "    pool_d = values.shape[1]\n",
    "    ip_ratio = image_d / pool_d\n",
    "    out = np.zeros([N,image_d,image_d])\n",
    "    out[gmi[:,0],gmi[:,1],gmi[:,2]] = 1\n",
    "    val_repeated = np.repeat(np.repeat(values,ip_ratio,axis=1),ip_ratio,axis=2)\n",
    "    return out * val_repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My backpropagation for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "class bornaCNN:\n",
    "    def __init__(self, mlp_layers_sizes, mlp_activations, CNN_activations,input_dimension ,num_of_kernels=5,kernels_dimensions=[5], \n",
    "                 max_pool_dimensions = [2] , batch_size=10, eta=0.05, epocs=1000, update='online', \n",
    "                 minibatch_percentage=0.1, verbose=True):\n",
    "        \n",
    "        self.minibatch_percentage = minibatch_percentage\n",
    "        self.update=update# other values batch, mini-batch\n",
    "        self.errors=[]\n",
    "        self.eta=eta\n",
    "        self.maxp_crop_lenghts=[]\n",
    "\n",
    "        self.epocs=epocs\n",
    "        self.num_of_kernels = num_of_kernels        \n",
    "        self.kernels_dimensions = kernels_dimensions\n",
    "        self.mlp_activations,self.CNN_activations=mlp_activations,CNN_activations\n",
    "        self.max_pool_dimensions = max_pool_dimensions\n",
    "        ######## mlp_layers_sizes doesn't contain the the number of nodes between the last maxp\n",
    "        # wecalculate this\n",
    "        \n",
    "        lcmld = input_dimension # lcmd = Last Conv-Max Layer Dimension\n",
    "        for i in range(len(kernels_dimensions)):\n",
    "            lcmld = lcmld - kernels_dimensions[i]+1\n",
    "            lcmld = lcmld / max_pool_dimensions[i]\n",
    "\n",
    "        self.mlp_layers_sizes = np.insert(mlp_layers_sizes,0,num_of_kernels*lcmld*lcmld) # does not contain the \n",
    "        ########\n",
    "        self.num_mlp_layers = len(self.mlp_layers_sizes )\n",
    "        self.verbose= verbose\n",
    "        self.kernels_W , self.kernels_B = self.generate_random_kernels()\n",
    "        self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.maxp_gmi = []# global max index of the maxpool layer. we use it to back propagate the deltas backward\n",
    "        self.num_of_kernels = num_of_kernels\n",
    "    \n",
    "    \n",
    "    def generate_random_kernels(self):#for now, we just use the same initialization technic that we use for MLP weights\n",
    "        kernels_W={}\n",
    "        kernels_B={}\n",
    "        for i,kd in enumerate(self.kernels_dimensions):# number of Convolutions layers\n",
    "            i = 2*i\n",
    "            #upper = 4*sqrt(6)/sqrt(self.layers_sizes[i]+self.layers_sizes[i+1])\n",
    "            upper = 4*sqrt(6)/sqrt(kd**2+1)\n",
    "            key = str(i)+'-'+str(i+1)\n",
    "            kernels_B[key] = np.zeros(self.num_of_kernels)\n",
    "            kernels_W[key] = np.random.uniform(-upper,upper,[self.num_of_kernels,kd,kd])\n",
    "        return kernels_W,kernels_B\n",
    "        \n",
    "    def generate_random_mlp_weights(self):\n",
    "        mlp_W={}\n",
    "        mlp_B={}\n",
    "        for i in range(len(self.mlp_layers_sizes)-1):\n",
    "            upper = 4*sqrt(6)/sqrt(self.mlp_layers_sizes[i]+self.mlp_layers_sizes[i+1])\n",
    "            #weight also includes biases\n",
    "            mlp_B[str(i)+\"-\"+str(i+1)]=np.zeros(self.mlp_layers_sizes[i+1])\n",
    "            mlp_W[str(i)+\"-\"+str(i+1)] = np.random.uniform(-upper,upper, self.mlp_layers_sizes[i:i+2])#np.ones( self.layers_sizes[i:i+2])\n",
    "        return mlp_W,mlp_B\n",
    "    \n",
    "    def _calc_error(self,X,Y):\n",
    "        prediction = self.feed_forward(X)\n",
    "        return .5*np.sum((Y-prediction[-1])**2)/(X.shape[0])\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        self.W , self.B= self.generate_random_weights()\n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "        \n",
    "        self.errors.append(self._calc_error(X,Y))# Error before we start training        \n",
    "        for i in range(self.epocs):\n",
    "            if self.verbose:\n",
    "                if i<10:\n",
    "                    print (\"epoc->\",i+1)\n",
    "                elif i<100 and i%10==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<1000 and i%100==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<10000 and i%1000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<100000 and i%10000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "            \n",
    "            for j in range(X.shape[0]):\n",
    "                delta_W, delta_B = self.calc_weight_updates(X[j,:],Y[j])\n",
    "                self.update_weights(delta_W, delta_B)\n",
    "            self.errors.append(self._calc_error(X,Y))\n",
    "                \n",
    "    def update_weights(self,delta_W,delta_B):\n",
    "        for i in range(self.num_layers-1): #going through layers\n",
    "            index=str(i)+\"-\"+str(i+1)\n",
    "            self.W[index] -= delta_W[index]\n",
    "            self.B[index] -= delta_B[index]\n",
    "\n",
    "    def predict(self,X):\n",
    "        nn_output = self.feed_forward(X)\n",
    "        labels = np.argmax(nn_output[-1],axis=1)\n",
    "        return labels\n",
    "        \n",
    "    def calc_weight_updates(self,inputx,target):\n",
    "        ffr = self.feed_forward(inputx)\n",
    "        # https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/        \n",
    "        #step1: output layer\n",
    "        delta_W={}\n",
    "        delta_B={}\n",
    "        \n",
    "        this_output = ffr[-1]\n",
    "        previous_output = ffr[-2]\n",
    "\n",
    "        #this_delta = ((this_output - target)) * (this_output * (1-this_output))\n",
    "        act_deriv = derivative(self.activations[self.num_layers-2])\n",
    "        this_delta = ((this_output - target)) * act_deriv(this_output) \n",
    "\n",
    "        weight_chagnge = np.outer(previous_output , this_delta)\n",
    "        \n",
    "        delta_W[str(self.num_layers-2)+\"-\"+str(self.num_layers-1)] = self.eta * weight_chagnge\n",
    "        delta_B[str(self.num_layers-2)+\"-\"+str(self.num_layers-1)] = self.eta *this_delta\n",
    "        \n",
    "        #step2: Hidden Layers\n",
    "        for i in reversed(range(1,self.num_layers-1)): # Going through all the layers backwards\n",
    "            next_layer_delta = this_delta\n",
    "            \n",
    "            hl_input = ffr[i-1]\n",
    "            hl_output = ffr[i]\n",
    "            hl_out_weights = self.W[str(i)+\"-\"+str(i+1)]\n",
    "            #this_delta =np.dot(hl_out_weights, next_layer_delta) * hl_output *(1-hl_output)\n",
    "            act_deriv = derivative(self.activations[i-1])\n",
    "            this_delta =np.dot(hl_out_weights, next_layer_delta) * act_deriv(hl_output)\n",
    "            \n",
    "            weight_chagnge = np.outer(hl_input,this_delta)\n",
    "            delta_W[str(i-1)+\"-\"+str(i)] = self.eta* weight_chagnge     \n",
    "            delta_B[str(i-1)+\"-\"+str(i)] = self.eta* this_delta\n",
    "            \n",
    "        # step3 iterating betwen Maxpool and conv steps and calculating the weight updates\n",
    "            \n",
    "        return delta_W, delta_B\n",
    "    \n",
    "    def feed_forward(self,x): #feed forward, x is a 2d matrix\n",
    "        x = np.array(x)        \n",
    "        ffr_K = []\n",
    "        layer_input = np.repeat(x[np.newaxis],self.num_of_kernels,axis=0)# make 2d inpu to k, 2d inpus\n",
    "        \n",
    "        for i,kd in enumerate(self.kernels_dimensions): #going through each Con layer and the max pool layer\n",
    "            # We do apply the kernels first\n",
    "            conv_layer_output = np.empty([self.num_of_kernels,layer_input.shape[1]-kd+1,layer_input.shape[1]-kd+1],dtype=float) # conv_output empty\n",
    "            key = str(2*i)+'-'+str(2*i+1)\n",
    "            Ks,Bs = self.kernels_W[key] , self.kernels_B[key]\n",
    "            for j in range(Ks.shape[0]):#iterating through the each kernel\n",
    "                conv_layer_output[j] = convolve2d(layer_input[j],Ks[j],'valid') #+ Bs[j]\n",
    "                ##CNN_activations!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                \n",
    "            ffr_K.append(conv_layer_output) #append the convolution\n",
    "            ## Max pool dimension\n",
    "            maxp_d = self.max_pool_dimensions[i]\n",
    "            crop_length, gmi, maxp_output = maxpool(conv_layer_output,maxp_d)\n",
    "            self.maxp_gmi.append(gmi)\n",
    "            self.maxp_crop_lenghts.append(crop_length)\n",
    "            \n",
    "            ffr_K.append(maxp_output)\n",
    "            layer_input = maxp_output # for the next iteration\n",
    "\n",
    "            \n",
    "            \n",
    "        # before passing on the last layer to the fully connect network, we need to flatten it: layer_input.reshape(-1)\n",
    "        #layer_input is actually the output of last layer before MLP\n",
    "        \n",
    "        x = layer_input.reshape(-1)\n",
    "        print \"x.shape\",x.shape\n",
    "        ffr_mlp=[x]#Feed Forward Result\n",
    "        for i in range(self.num_mlp_layers-1):\n",
    "            W = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "            B = self.mlp_B[str(i)+\"-\"+str(i+1)]\n",
    "            y = self.mlp_activations[i](np.dot(x,W) + B) #clculate the output of the layer\n",
    "            x=y\n",
    "            ffr_mlp.append(y)\n",
    "        return ffr_K,gmi,ffr_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bornacnn = bornaCNN( mlp_layers_sizes=[5,2], mlp_activations=[sigmoid,sigmoid], CNN_activations = [sigmoid,sigmoid],input_dimension = 500 ,\n",
    "                    num_of_kernels=2,kernels_dimensions=[2,2], max_pool_dimensions = [2,2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (30752L,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(500,500)\n",
    "ffr_K,gmi,ffr_mlp = bornacnn.feed_forward(x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
