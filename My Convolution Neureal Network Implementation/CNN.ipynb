{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own BackPropagation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#let's define some basic functions. Even though I am not using anything besides sigmoid function\n",
    "#, I have defined the softmax and the derivative \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(np.zeros(x.shape),x)\n",
    "\n",
    "def relu_derivative(relu_x):\n",
    "    relu_x[relu_x>0] = 1\n",
    "    relu_x[relu_x<=0] = 0\n",
    "    return relu_x\n",
    "    \n",
    "\n",
    "def sigmoid_derivative(sigmoid_x): # we calculate the derivative based on the sigmoid function value\n",
    "    return sigmoid_x*(1-sigmoid_x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1-x**2\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid:\n",
    "        return sigmoid_derivative\n",
    "    elif f == np.tanh:\n",
    "        return tanh_derivative\n",
    "    elif f == relu:\n",
    "        return relu_derivative\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "rot180 = lambda X: np.rot90(X,2)\n",
    "    \n",
    "def maxpool(x,pool_d):\n",
    "    \n",
    "    def local_max_indices(x,pool_d): #if is not devidable wihch can happen as a result of conv, we need to do something about it\n",
    "        \"\"\"Return maximum in groups of pool_dxpool_d for a N,h,w image\"\"\"\n",
    "        N,h,w = x.shape\n",
    "        x = x.reshape(N,h/pool_d,pool_d,w/pool_d,pool_d).swapaxes(2,3).reshape(N,h/pool_d,w/pool_d,pool_d*pool_d)\n",
    "        return np.argmax(x,axis=3)    \n",
    "    \n",
    "    def global_max_indices(x,pool_d):\n",
    "        N = x.shape[0]\n",
    "        image_d= x.shape[1]\n",
    "        ip_ratio = image_d / pool_d\n",
    "        lmi = local_max_indices(x,pool_d)\n",
    "        max_local_x,max_local_y = np.unravel_index(lmi.flat,dims=(pool_d,pool_d))\n",
    "        max_y =  max_local_y + np.tile(np.tile(range(ip_ratio),ip_ratio)*pool_d,N)\n",
    "        max_x =  max_local_x + np.tile(np.repeat(np.arange(ip_ratio), ip_ratio)*pool_d,N)\n",
    "        Ns = np.repeat(np.arange(N),ip_ratio**2)\n",
    "        return np.vstack([Ns,max_x,max_y]).T    \n",
    "    \n",
    "    N= x.shape[0]\n",
    "    image_d = x.shape[1]\n",
    "    crop_length = image_d%pool_d\n",
    "    x = x[:,:image_d-crop_length,:image_d-crop_length]\n",
    "    gmi = global_max_indices(x,pool_d)\n",
    "    maxes =  x[gmi[:,0],gmi[:,1],gmi[:,2]].reshape(N,image_d/pool_d,image_d/pool_d)\n",
    "    return gmi,maxes\n",
    "\n",
    "def up_sample(gmi,values,image_d,pool_d):\n",
    "    N =values.shape[0] \n",
    "    out = np.zeros([N,image_d,image_d])\n",
    "    out[gmi[:,0],gmi[:,1],gmi[:,2]] = 1 #maxes are equal to one\n",
    "    val_repeated = np.repeat(np.repeat(values,pool_d,axis=1),pool_d,axis=2)\n",
    "    crop_length = image_d - val_repeated.shape[1]\n",
    "    val_repeated = np.pad(val_repeated, ((0,0),(0,crop_length), (0,crop_length)), mode='constant', constant_values=0) # pad with zero to reverse cropping \n",
    "    return out * val_repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_images = pd.read_csv(\"./train.csv\")\n",
    "test_images = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "train_images_numpy = train_images[train_images.columns[1:]].as_matrix().astype(float)\n",
    "test_images_numpy = test_images[train_images.columns[1:]].as_matrix().astype(float)\n",
    "\n",
    "from sklearn.preprocessing import scale,LabelBinarizer\n",
    "train_scaled  = scale(train_images_numpy)\n",
    "test_scaled  = scale(test_images_numpy)\n",
    "\n",
    "X = train_scaled.reshape(-1,28,28)# making the images square before feeding it to the CNN\n",
    "X_test = test_scaled.reshape(-1,28,28)# making the images square before feeding it to the CNN\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "Y= lb.fit_transform(train_images['label'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My backpropagation for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# implementation based on : http://cogprints.org/5869/1/cnn_tutorial.pdf\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "class bornaCNN:\n",
    "    def __init__(self, mlp_layers_sizes, mlp_activations, CNN_activations,input_dimension ,num_of_kernels=5\n",
    "                 ,kernels_dimensions=[5], max_pool_dimensions = [2] , eta=0.05, epocs=5, update='online', verbose=True):\n",
    "        \n",
    "        self.errors=[]\n",
    "        self.eta=eta\n",
    "\n",
    "        self.epocs=epocs\n",
    "        self.num_of_kernels = num_of_kernels        \n",
    "        self.kernels_dimensions = kernels_dimensions\n",
    "        self.mlp_activations,self.CNN_activations = mlp_activations,CNN_activations\n",
    "        self.max_pool_dimensions = max_pool_dimensions\n",
    "        ######## mlp_layers_sizes doesn't contain the the number of nodes between the last maxp\n",
    "        # we calculate this->\n",
    "        \n",
    "        lcmld = input_dimension # lcmd = Last Conv-Max Layer Dimension\n",
    "        for i in range(len(kernels_dimensions)):\n",
    "            lcmld = lcmld - kernels_dimensions[i]+1\n",
    "            lcmld = lcmld / max_pool_dimensions[i]\n",
    "\n",
    "        self.mlp_layers_sizes = np.insert(mlp_layers_sizes,0,num_of_kernels*lcmld*lcmld) # does not contain the \n",
    "        #####################\n",
    "        self.num_mlp_layers = len(self.mlp_layers_sizes )\n",
    "        self.verbose= verbose\n",
    "        self.kernels_W , self.kernels_B = self.generate_random_kernels()\n",
    "        #self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.num_of_kernels = num_of_kernels\n",
    "    \n",
    "    def generate_random_kernels(self):#for now, we just use the same initialization technic that we use for MLP weights\n",
    "        kernels_W={}\n",
    "        kernels_B={}\n",
    "        for i,kd in enumerate(self.kernels_dimensions):# number of Convolutions layers\n",
    "            i = 2*i\n",
    "            #upper = 4*sqrt(6)/sqrt(self.layers_sizes[i]+self.layers_sizes[i+1])\n",
    "            #upper = 4*sqrt(6)/sqrt(kd**2+1)\n",
    "            key = str(i)+'-'+str(i+1)\n",
    "            kernels_B[key] = np.zeros(self.num_of_kernels)\n",
    "            #kernels_W[key] = np.random.uniform(-upper,upper,[self.num_of_kernels,kd,kd])\n",
    "            kernels_W[key] = np.random.uniform(-3.,3.,[self.num_of_kernels,kd,kd])\n",
    "        return kernels_W,kernels_B\n",
    "        \n",
    "    def generate_random_mlp_weights(self):\n",
    "        mlp_W={}\n",
    "        mlp_B={}\n",
    "        for i in range(len(self.mlp_layers_sizes)-1):\n",
    "            upper = 4*sqrt(6)/sqrt(self.mlp_layers_sizes[i]+self.mlp_layers_sizes[i+1])\n",
    "            mlp_B[str(i)+\"-\"+str(i+1)] = np.zeros(self.mlp_layers_sizes[i+1])\n",
    "            mlp_W[str(i)+\"-\"+str(i+1)] = np.random.uniform(-upper,upper, self.mlp_layers_sizes[i:i+2])#np.ones( self.layers_sizes[i:i+2])\n",
    "        return mlp_W,mlp_B\n",
    "    \n",
    "    def _calc_error(self,X,Y):\n",
    "        prediction = np.zeros(Y.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            ffr_K,gmi,ffr_mlp = self.feed_forward(X[i])\n",
    "            prediction[i] = ffr_mlp[-1]\n",
    "        error = .5*np.sum((Y-prediction)**2)/(X.shape[0])\n",
    "        print \"Error->\",error\n",
    "        return error\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        self.errors=[]\n",
    "        self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.errors.append(self._calc_error(X,Y))# Error before we start training        \n",
    "        for i in range(self.epocs):\n",
    "            if self.verbose:\n",
    "                if i<10:\n",
    "                    print \"epoc->\",i\n",
    "                elif i<100 and i%10==0:\n",
    "                    print \"epoc->\",i\n",
    "                elif i<1000 and i%100==0:\n",
    "                    print \"epoc->\",i\n",
    "                elif i<10000 and i%1000==0:\n",
    "                    print \"epoc->\",i\n",
    "                elif i<100000 and i%10000==0:\n",
    "                    print \"epoc->\",i\n",
    "            \n",
    "            for j in range(X.shape[0]):\n",
    "                inputx = X[j,:,:]\n",
    "                target = Y[j,:]\n",
    "                ffr_K,all_gmi,ffr_mlp = self.feed_forward(inputx)\n",
    "                \n",
    "                first_delta, delta_MLP_W, delta_MLP_B = self.calc_weight_updates_MLP(ffr_mlp,target)\n",
    "                self.update_weights(delta_MLP_W, delta_MLP_B,\"mlp\")\n",
    "                \n",
    "                delta_K_W, delta_K_B =  self.calc_weight_updates_K(ffr_K , all_gmi , first_delta)\n",
    "                self.update_weights(delta_K_W, delta_K_B,\"kernel\")\n",
    "                \n",
    "            self.errors.append(self._calc_error(X,Y))\n",
    "                \n",
    "    def update_weights(self,delta_W,delta_B,layer_type):\n",
    "        if layer_type.lower() == 'mlp':\n",
    "            for i in range(self.num_mlp_layers-1): #going through layers\n",
    "                index=str(i)+\"-\"+str(i+1)\n",
    "                self.mlp_W[index] -= self.eta * delta_W[index]\n",
    "                self.mlp_B[index] -= self.eta * delta_B[index]\n",
    "        else: #conv\n",
    "            for key in delta_W:\n",
    "                self.kernels_W[key] -= self.eta * delta_W[key]\n",
    "                self.kernels_B[key] -= self.eta * delta_B[key]\n",
    "\n",
    "    def predict(self,X):\n",
    "        labels = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            ffr_K,gmi,ffr_mlp = bornacnn.feed_forward(X[i])\n",
    "            labels[i] = np.argmax(ffr_mlp[-1])\n",
    "        return labels\n",
    "    \n",
    "    def calc_weight_updates_MLP(self,ffr_mlp,target):\n",
    "        # https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/        \n",
    "        #step1: output layer\n",
    "        delta_W={}\n",
    "        delta_B={}\n",
    "        \n",
    "        this_output = ffr_mlp[-1]\n",
    "        previous_output = ffr_mlp[-2]\n",
    "\n",
    "        act_deriv = derivative(self.mlp_activations[self.num_mlp_layers-2])\n",
    "        this_delta = ((this_output - target)) * act_deriv(this_output) \n",
    "\n",
    "        weight_chagnge = np.outer(previous_output , this_delta)\n",
    "        \n",
    "        delta_W[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = weight_chagnge\n",
    "        delta_B[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = this_delta\n",
    "        \n",
    "        #step2: Hidden Layers\n",
    "        for i in reversed(range(0,self.num_mlp_layers-1)): # Going through all the layers backwards* changed range(1 to range(0\n",
    "            next_layer_delta = this_delta\n",
    "            hl_out_weights = self.mlp_W[str(i)+\"-\"+str(i+1)]            \n",
    "            \n",
    "            if i>0:\n",
    "                hl_input = ffr_mlp[i-1]\n",
    "                hl_output = ffr_mlp[i]\n",
    "                act_deriv = derivative(self.mlp_activations[i-1])\n",
    "                this_delta = np.dot(hl_out_weights, next_layer_delta) * act_deriv(hl_output)\n",
    "                weight_chagnge = np.outer(hl_input,this_delta)\n",
    "                delta_W[str(i-1)+\"-\"+str(i)] =  weight_chagnge     \n",
    "                delta_B[str(i-1)+\"-\"+str(i)] =  this_delta\n",
    "            else:#i ==0 input layer's output is f(x)=x\n",
    "                hl_out_weights = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "                first_delta = np.dot(hl_out_weights, next_layer_delta) # this part is equal to 1* act_deriv(hl_output)\n",
    "            \n",
    "        return first_delta, delta_W, delta_B\n",
    "    \n",
    "    def feed_forward(self,x): #feed forward, x is a 2d image matrix\n",
    "        x = np.array(x)        \n",
    "        ffr_K = []\n",
    "        all_gmi=[]\n",
    "        layer_input = np.repeat(x[np.newaxis],self.num_of_kernels,axis=0)# make 2d inpu to k, 2d inpus\n",
    "        ffr_K.append(layer_input)\n",
    "        \n",
    "        for i,kd in enumerate(self.kernels_dimensions): #going through each Con layer and the max pool layer\n",
    "            \n",
    "            # We do apply the kernels first\n",
    "            conv_layer_output = np.zeros([self.num_of_kernels,layer_input.shape[1]-kd+1,layer_input.shape[1]-kd+1],dtype=float) # conv_output empty\n",
    "            key = str(2*i)+'-'+str(2*i+1)\n",
    "            Ks,Bs = self.kernels_W[key] , self.kernels_B[key]\n",
    "            for j in range(Ks.shape[0]):#iterating through the each kernel\n",
    "                conv_layer_output[j] = convolve2d(layer_input[j],Ks[j],'valid') + Bs[j]\n",
    "            conv_layer_output = self.CNN_activations[i](conv_layer_output) #applying the activation function\n",
    "            ffr_K.append(conv_layer_output) #append the convolution\n",
    "            ## Max pool dimension\n",
    "            maxp_d = self.max_pool_dimensions[i]\n",
    "            gmi, maxp_output = maxpool(conv_layer_output,maxp_d)\n",
    "            all_gmi.append(gmi)\n",
    "            ffr_K.append(maxp_output)\n",
    "            layer_input = maxp_output # for the next iteration\n",
    "\n",
    "        # before passing on the last layer to the fully connect network, we need to flatten it: layer_input.reshape(-1)\n",
    "        #layer_input is actually the output of last layer before MLP\n",
    "        \n",
    "        x = layer_input.reshape(-1)#flatten the 2d maxpool output\n",
    "        ffr_mlp=[x]#Feed Forward Result\n",
    "        for i in range(self.num_mlp_layers-1):\n",
    "            W = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "            B = self.mlp_B[str(i)+\"-\"+str(i+1)]\n",
    "            y = self.mlp_activations[i](np.dot(x,W) + B) #clculate the output of the layer\n",
    "            x=y\n",
    "            ffr_mlp.append(y)\n",
    "        return ffr_K,all_gmi,ffr_mlp\n",
    "    \n",
    "    def calc_weight_updates_K(self,ffr_K,gmi,first_delta):\n",
    "        detla_k_W={}\n",
    "        detla_k_B={}\n",
    "       \n",
    "        for i in reversed(range(len(self.kernels_dimensions))):\n",
    "            #first the maxpool delta - No training for maxpool layer\n",
    "            maxpool_delta = np.zeros(ffr_K[2*i+2].shape)\n",
    "            if i == len(self.kernels_dimensions)-1: #if it is the first before the fully connected, there is how the delta is calculated\n",
    "                delta_dim = int(sqrt(first_delta.shape[0] / self.num_of_kernels))\n",
    "                maxpool_delta = first_delta.reshape(self.num_of_kernels,delta_dim,delta_dim)\n",
    "            else:\n",
    "                Ks = self.kernels_W[str(2*(i+1))+'-'+str(2*(i+1)+1)]\n",
    "                for j in range(conv_delta.shape[0]):\n",
    "                    maxpool_delta[j] = convolve2d(conv_delta[j], rot180(Ks[j]),'full')\n",
    "            \n",
    "            #conv layer delta\n",
    "           \n",
    "            image_d = ffr_K[2*i+1].shape[1]\n",
    "            cnn_derivative = derivative(self.CNN_activations[i])\n",
    "            conv_delta = cnn_derivative(ffr_K[2*i+1]) * up_sample(gmi[i],maxpool_delta ,image_d, self.max_pool_dimensions[i])\n",
    "\n",
    "            key = str(2*i)+'-'+str(2*i+1)\n",
    "            \n",
    "            detla_k_W[key] = np.zeros(self.kernels_W[key].shape) \n",
    "            detla_k_B[key] = np.zeros(self.kernels_B[key].shape) \n",
    "            for j in range(self.num_of_kernels):\n",
    "                this_k = self.kernels_W[key][j]\n",
    "                detla_k_W[key][j] = rot180(convolve2d(ffr_K[2*i][j],rot180(conv_delta[j]),\"valid\")) \n",
    "                detla_k_B[key][j] = np.sum(conv_delta[j])\n",
    "                \n",
    "        return detla_k_W,detla_k_B    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's train our CNN to classify MNIST digits\n",
    "### We have 1 Convolutional Layer (12 kernels) and one max pool layer. \n",
    "### Fully connected layers has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error-> 1.60781704289\n",
      "epoc-> 0\n",
      "Error-> 0.0461810810899\n",
      "epoc-> 1\n",
      "Error-> 0.0350716276636\n",
      "epoc-> 2\n",
      "Error-> 0.0252425886514\n",
      "epoc-> 3\n",
      "Error-> 0.0247052112233\n",
      "epoc-> 4\n",
      "Error-> 0.0184696363564\n",
      "epoc-> 5\n",
      "Error-> 0.0171006207499\n",
      "epoc-> 6\n",
      "Error-> 0.0139339252701\n",
      "epoc-> 7\n",
      "Error-> 0.0121758959527\n",
      "epoc-> 8\n",
      "Error-> 0.00997731832315\n",
      "epoc-> 9\n",
      "Error-> 0.00934495507682\n",
      "epoc-> 10\n",
      "Error-> 0.00672947839294\n",
      "Error-> 0.00630731131009\n",
      "Error-> 0.00520819289832\n",
      "Error-> 0.00554094137322\n",
      "Error-> 0.00475233657901\n",
      "Error-> 0.00369676434586\n",
      "Error-> 0.00340611395534\n",
      "Error-> 0.00325565920701\n",
      "Error-> 0.00263824437318\n",
      "Error-> 0.00227178879129\n",
      "epoc-> 20\n",
      "Error-> 0.00201914852037\n",
      "Error-> 0.00193852528424\n",
      "Error-> 0.00186829940873\n",
      "Error-> 0.00164218655001\n",
      "Error-> 0.00154096861654\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSepjMEiS+vx/jkh/S0jbb6YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc983828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bornacnn = bornaCNN(mlp_layers_sizes=[80,80,10], \n",
    "                    mlp_activations=[sigmoid,sigmoid,sigmoid], \n",
    "                    CNN_activations = [sigmoid,sigmoid],input_dimension = 28 ,\n",
    "                    num_of_kernels=12,kernels_dimensions=[5], max_pool_dimensions = [2],\n",
    "                    epocs=25, eta = 0.1)\n",
    "bornacnn.fit(X,Y) \n",
    "%matplotlib inline  \n",
    "plt.plot(range(len(bornacnn.errors)),bornacnn.errors)\n",
    "plt.xlabel('Epoc')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "# How to improve my CNN -> http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99804761904761907"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = bornacnn.predict(X)\n",
    "sum(predictions == train_images.loc[:]['label'].as_matrix()) / float(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = bornacnn.predict(X_test)\n",
    "test_predictions = np.vstack([range(1,len(test_predictions)+1),test_predictions.astype(int)]) # attaching ImageIds\n",
    "my_cnn_submission = pd.DataFrame(test_predictions.T,columns=['ImageId','Label'])\n",
    "my_cnn_submission.to_csv('my_cnn_submission-5-2-808010-sigmoid-sigmoid.csv',index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The above submission got 97.886 percent accuracy on test data!! Yeaaaahhh!!!! \n",
    "# Not the best result, but hey I have implemented my own convolutional neureal network  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
