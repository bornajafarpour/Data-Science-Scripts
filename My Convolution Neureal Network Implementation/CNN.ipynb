{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own BackPropagation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#let's define some basic functions. Even though I am not using anything besides sigmoid function\n",
    "#, I have defined the softmax and the derivative \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(np.zeros(x.shape[0]),x)\n",
    "\n",
    "def identity_derivative(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def sigmoid_derivative(sigmoid_x): # we calculate the derivative based on the sigmoid function value\n",
    "    return sigmoid_x*(1-sigmoid_x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1-x**2\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid:\n",
    "        return sigmoid_derivative\n",
    "    elif f == np.tanh:\n",
    "        return tanh_derivative\n",
    "    elif f == identity:\n",
    "        return identity_derivative\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "rot180 = lambda X: np.rot90(X,2)\n",
    "    \n",
    "def maxpool(x,pool_d):\n",
    "    \n",
    "    def local_max_indices(x,pool_d): #if is not devidable wihch can happen as a result of conv, we need to do something about it\n",
    "        \"\"\"Return maximum in groups of pool_dxpool_d for a N,h,w image\"\"\"\n",
    "        N,h,w = x.shape\n",
    "        x = x.reshape(N,h/pool_d,pool_d,w/pool_d,pool_d).swapaxes(2,3).reshape(N,h/pool_d,w/pool_d,pool_d*pool_d)\n",
    "        return np.argmax(x,axis=3)    \n",
    "    \n",
    "    def global_max_indices(x,pool_d):\n",
    "        N = x.shape[0]\n",
    "        image_d= x.shape[1]\n",
    "        ip_ratio = image_d / pool_d\n",
    "        lmi = local_max_indices(x,pool_d)\n",
    "        max_local_x,max_local_y = np.unravel_index(lmi.flat,dims=(pool_d,pool_d))\n",
    "        max_y =  max_local_y + np.tile(np.tile(range(ip_ratio),ip_ratio)*pool_d,N)\n",
    "        max_x =  max_local_x + np.tile(np.repeat(np.arange(ip_ratio), ip_ratio)*pool_d,N)\n",
    "        Ns = np.repeat(np.arange(N),ip_ratio**2)\n",
    "        return np.vstack([Ns,max_x,max_y]).T    \n",
    "    \n",
    "    N= x.shape[0]\n",
    "    image_d = x.shape[1]\n",
    "    crop_length = image_d%pool_d\n",
    "    x = x[:,:image_d-crop_length,:image_d-crop_length]\n",
    "    gmi = global_max_indices(x,pool_d)\n",
    "    maxes =  x[gmi[:,0],gmi[:,1],gmi[:,2]].reshape(N,image_d/pool_d,image_d/pool_d)\n",
    "    return gmi,maxes\n",
    "\n",
    "def up_sample(gmi,values,image_d,pool_d):\n",
    "    N =values.shape[0] \n",
    "    out = np.zeros([N,image_d,image_d])\n",
    "    out[gmi[:,0],gmi[:,1],gmi[:,2]] = 1 #maxes are equal to one\n",
    "    val_repeated = np.repeat(np.repeat(values,pool_d,axis=1),pool_d,axis=2)\n",
    "    crop_length = image_d - val_repeated.shape[1]\n",
    "    val_repeated = np.pad(val_repeated, ((0,0),(0,crop_length), (0,crop_length)), mode='constant', constant_values=0) # pad with zero to reverse cropping \n",
    "    return out * val_repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_images = pd.read_csv(\"./train.csv\")\n",
    "#train_images = train_images[:10000]\n",
    "# test_images = pd.read_csv(\"./test.csv\")\n",
    "#train_images = train_images.loc[1:1000]\n",
    "\n",
    "train_images_numpy = train_images[train_images.columns[1:]].as_matrix().astype(float)\n",
    "from sklearn.preprocessing import scale,LabelBinarizer\n",
    "train_scaled  = scale(train_images_numpy)\n",
    "\n",
    "X = train_scaled.reshape(-1,28,28)# making the images square before feeding it to the CNN\n",
    "lb = LabelBinarizer()\n",
    "Y= lb.fit_transform(train_images['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My backpropagation for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "class bornaCNN:\n",
    "    def __init__(self, mlp_layers_sizes, mlp_activations, CNN_activations,input_dimension ,num_of_kernels=5\n",
    "                 ,kernels_dimensions=[5], max_pool_dimensions = [2] , eta=0.05, epocs=5, update='online', verbose=True):\n",
    "        \n",
    "        self.errors=[]\n",
    "        self.eta=eta\n",
    "\n",
    "        self.epocs=epocs\n",
    "        self.num_of_kernels = num_of_kernels        \n",
    "        self.kernels_dimensions = kernels_dimensions\n",
    "        self.mlp_activations,self.CNN_activations = mlp_activations,CNN_activations\n",
    "        self.max_pool_dimensions = max_pool_dimensions\n",
    "        ######## mlp_layers_sizes doesn't contain the the number of nodes between the last maxp\n",
    "        # we calculate this->\n",
    "        \n",
    "        lcmld = input_dimension # lcmd = Last Conv-Max Layer Dimension\n",
    "        for i in range(len(kernels_dimensions)):\n",
    "            lcmld = lcmld - kernels_dimensions[i]+1\n",
    "            lcmld = lcmld / max_pool_dimensions[i]\n",
    "\n",
    "        self.mlp_layers_sizes = np.insert(mlp_layers_sizes,0,num_of_kernels*lcmld*lcmld) # does not contain the \n",
    "        #####################\n",
    "        self.num_mlp_layers = len(self.mlp_layers_sizes )\n",
    "        self.verbose= verbose\n",
    "        self.kernels_W , self.kernels_B = self.generate_random_kernels()\n",
    "        #self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.num_of_kernels = num_of_kernels\n",
    "    \n",
    "    \n",
    "    def generate_random_kernels(self):#for now, we just use the same initialization technic that we use for MLP weights\n",
    "        kernels_W={}\n",
    "        kernels_B={}\n",
    "        for i,kd in enumerate(self.kernels_dimensions):# number of Convolutions layers\n",
    "            i = 2*i\n",
    "            #upper = 4*sqrt(6)/sqrt(self.layers_sizes[i]+self.layers_sizes[i+1])\n",
    "            upper = 4*sqrt(6)/sqrt(kd**2+1)\n",
    "            key = str(i)+'-'+str(i+1)\n",
    "            kernels_B[key] = np.zeros(self.num_of_kernels)\n",
    "            #kernels_W[key] = np.random.uniform(-upper,upper,[self.num_of_kernels,kd,kd])\n",
    "            kernels_W[key] = np.random.uniform(-3.,3.,[self.num_of_kernels,kd,kd])\n",
    "        return kernels_W,kernels_B\n",
    "        \n",
    "    def generate_random_mlp_weights(self):\n",
    "        mlp_W={}\n",
    "        mlp_B={}\n",
    "        for i in range(len(self.mlp_layers_sizes)-1):\n",
    "            upper = 4*sqrt(6)/sqrt(self.mlp_layers_sizes[i]+self.mlp_layers_sizes[i+1])\n",
    "            #weight also includes biases\n",
    "            mlp_B[str(i)+\"-\"+str(i+1)] = np.zeros(self.mlp_layers_sizes[i+1])\n",
    "            mlp_W[str(i)+\"-\"+str(i+1)] = np.random.uniform(-upper,upper, self.mlp_layers_sizes[i:i+2])#np.ones( self.layers_sizes[i:i+2])\n",
    "        return mlp_W,mlp_B\n",
    "    \n",
    "    def _calc_error(self,X,Y):\n",
    "        prediction = np.zeros(Y.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            ffr_K,gmi,ffr_mlp = self.feed_forward(X[i])\n",
    "            prediction[i] = ffr_mlp[-1]\n",
    "        error = .5*np.sum((Y-prediction)**2)/(X.shape[0])\n",
    "        print \"Error->\",error\n",
    "        return error\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        self.errors=[]\n",
    "        self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.errors.append(self._calc_error(X,Y))# Error before we start training        \n",
    "        for i in range(self.epocs):\n",
    "            if self.verbose:\n",
    "                if i<10:\n",
    "                    print (\"epoc->\",i+1)\n",
    "                elif i<100 and i%10==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<1000 and i%100==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<10000 and i%1000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<100000 and i%10000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "            \n",
    "            for j in range(X.shape[0]):\n",
    "                inputx = X[j,:,:]\n",
    "                target = Y[j,:]\n",
    "                ffr_K,all_gmi,ffr_mlp = self.feed_forward(inputx)\n",
    "                \n",
    "                first_delta, delta_MLP_W, delta_MLP_B = self.calc_weight_updates_MLP(ffr_mlp,target)\n",
    "                self.update_weights(delta_MLP_W, delta_MLP_B,\"mlp\")\n",
    "                \n",
    "                delta_K_W, delta_K_B =              self.calc_weight_updates_K(ffr_K , all_gmi , first_delta)\n",
    "                self.update_weights(delta_K_W, delta_K_B,\"kernel\")\n",
    "                \n",
    "                \n",
    "                \n",
    "            self.errors.append(self._calc_error(X,Y))\n",
    "                \n",
    "    def update_weights(self,delta_W,delta_B,layer_type):\n",
    "        \n",
    "        if layer_type.lower() == 'mlp':\n",
    "            for i in range(self.num_mlp_layers-1): #going through layers\n",
    "                index=str(i)+\"-\"+str(i+1)\n",
    "                self.mlp_W[index] -= self.eta * delta_W[index]\n",
    "                self.mlp_B[index] -= self.eta * delta_B[index]\n",
    "        else: #conv\n",
    "            for key in delta_W:\n",
    "                self.kernels_W[key] -= self.eta * delta_W[key]\n",
    "                self.kernels_B[key] -= self.eta * delta_B[key]\n",
    "\n",
    "#     def predict(self,X):\n",
    "#         nn_output = self.feed_forward(X)\n",
    "#         labels = np.argmax(nn_output[-1],axis=1)\n",
    "#         return labels\n",
    "    \n",
    "    def calc_weight_updates_MLP(self,ffr_mlp,target):\n",
    "        # https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/        \n",
    "        #step1: output layer\n",
    "        delta_W={}\n",
    "        delta_B={}\n",
    "        \n",
    "        this_output = ffr_mlp[-1]\n",
    "        previous_output = ffr_mlp[-2]\n",
    "\n",
    "        act_deriv = derivative(self.mlp_activations[self.num_mlp_layers-2])\n",
    "        this_delta = ((this_output - target)) * act_deriv(this_output) \n",
    "\n",
    "        weight_chagnge = np.outer(previous_output , this_delta)\n",
    "        \n",
    "        delta_W[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = weight_chagnge\n",
    "        delta_B[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = this_delta\n",
    "        \n",
    "        #step2: Hidden Layers\n",
    "        for i in reversed(range(0,self.num_mlp_layers-1)): # Going through all the layers backwards* changed range(1 to range(0\n",
    "            next_layer_delta = this_delta\n",
    "            hl_out_weights = self.mlp_W[str(i)+\"-\"+str(i+1)]            \n",
    "            \n",
    "            if i>0:\n",
    "                hl_input = ffr_mlp[i-1]\n",
    "                hl_output = ffr_mlp[i]\n",
    "                act_deriv = derivative(self.mlp_activations[i-1])\n",
    "                this_delta = np.dot(hl_out_weights, next_layer_delta) * act_deriv(hl_output)\n",
    "                weight_chagnge = np.outer(hl_input,this_delta)\n",
    "                delta_W[str(i-1)+\"-\"+str(i)] =  weight_chagnge     \n",
    "                delta_B[str(i-1)+\"-\"+str(i)] =  this_delta\n",
    "            else:#i ==0 input layer's output is f(x)=x\n",
    "                hl_out_weights = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "                first_delta = np.dot(hl_out_weights, next_layer_delta) # this part is equal to 1* act_deriv(hl_output)\n",
    "            \n",
    "        return first_delta, delta_W, delta_B\n",
    "    \n",
    "    def feed_forward(self,x): #feed forward, x is a 2d image matrix\n",
    "        x = np.array(x)        \n",
    "        ffr_K = []\n",
    "        all_gmi=[]\n",
    "        layer_input = np.repeat(x[np.newaxis],self.num_of_kernels,axis=0)# make 2d inpu to k, 2d inpus\n",
    "        ffr_K.append(layer_input)\n",
    "        \n",
    "        for i,kd in enumerate(self.kernels_dimensions): #going through each Con layer and the max pool layer\n",
    "            \n",
    "            # We do apply the kernels first\n",
    "            conv_layer_output = np.zeros([self.num_of_kernels,layer_input.shape[1]-kd+1,layer_input.shape[1]-kd+1],dtype=float) # conv_output empty\n",
    "            key = str(2*i)+'-'+str(2*i+1)\n",
    "            Ks,Bs = self.kernels_W[key] , self.kernels_B[key]\n",
    "            for j in range(Ks.shape[0]):#iterating through the each kernel\n",
    "                conv_layer_output[j] = convolve2d(layer_input[j],Ks[j],'valid') + Bs[j]\n",
    "            conv_layer_output = self.CNN_activations[i](conv_layer_output) #applying the activation function\n",
    "            ffr_K.append(conv_layer_output) #append the convolution\n",
    "            ## Max pool dimension\n",
    "            maxp_d = self.max_pool_dimensions[i]\n",
    "            gmi, maxp_output = maxpool(conv_layer_output,maxp_d)\n",
    "            all_gmi.append(gmi)\n",
    "            ffr_K.append(maxp_output)\n",
    "            layer_input = maxp_output # for the next iteration\n",
    "\n",
    "        # before passing on the last layer to the fully connect network, we need to flatten it: layer_input.reshape(-1)\n",
    "        #layer_input is actually the output of last layer before MLP\n",
    "        \n",
    "        x = layer_input.reshape(-1)#flatten the 2d maxpool output\n",
    "        ffr_mlp=[x]#Feed Forward Result\n",
    "        for i in range(self.num_mlp_layers-1):\n",
    "            W = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "            B = self.mlp_B[str(i)+\"-\"+str(i+1)]\n",
    "            y = self.mlp_activations[i](np.dot(x,W) + B) #clculate the output of the layer\n",
    "            x=y\n",
    "            ffr_mlp.append(y)\n",
    "        return ffr_K,all_gmi,ffr_mlp\n",
    "    \n",
    "    def calc_weight_updates_K(self,ffr_K,gmi,first_delta):\n",
    "        detla_k_W={}\n",
    "        detla_k_B={}\n",
    "        delta_dim = int(sqrt(first_delta.shape[0] / self.num_of_kernels))\n",
    "       \n",
    "        maxpool_delta = first_delta.reshape(self.num_of_kernels,delta_dim,delta_dim)\n",
    "        \n",
    "        for i in reversed(range(len(self.kernels_dimensions))):\n",
    "            cnn_derivative = derivative(self.CNN_activations[i])\n",
    "            #first the maxpool\n",
    "            image_d = ffr_K[i+1].shape[1]\n",
    "            conv_delta = up_sample(gmi[i],maxpool_delta ,image_d, self.max_pool_dimensions[i])\n",
    "            conv_delta = conv_delta*cnn_derivative(ffr_K[2*i+1])\n",
    "\n",
    "            #conv layer delta\n",
    "            key = str(2*i)+'-'+str(2*i+1)\n",
    "            \n",
    "            detla_k_W[key] = np.zeros(self.kernels_W[key].shape) \n",
    "            detla_k_B[key] = np.zeros(self.kernels_B[key].shape) \n",
    "            for j in range(self.num_of_kernels):\n",
    "                this_k = self.kernels_W[key][j]\n",
    "                detla_k_W[key][j] = rot180(convolve2d(ffr_K[2*i][j],rot180(conv_delta[j]),\"valid\")) \n",
    "                detla_k_B[key][j] = np.sum(conv_delta[j])\n",
    "                \n",
    "        return detla_k_W,detla_k_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error-> 1.31568934027\n",
      "('epoc->', 1)\n",
      "Error-> 0.0630383170412\n",
      "('epoc->', 2)\n",
      "Error-> 0.046176861601\n",
      "('epoc->', 3)\n",
      "Error-> 0.0342127962648\n",
      "('epoc->', 4)\n",
      "Error-> 0.0293736574007\n",
      "('epoc->', 5)\n",
      "Error-> 0.0274794815292\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d343828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "bornacnn = bornaCNN(mlp_layers_sizes=[20,10], \n",
    "                    mlp_activations=[sigmoid,sigmoid,sigmoid,sigmoid,sigmoid], \n",
    "                    CNN_activations = [sigmoid,sigmoid],input_dimension = 28 ,\n",
    "                    num_of_kernels=5,kernels_dimensions=[5], max_pool_dimensions = [2],\n",
    "                    epocs=5, eta = 0.15)\n",
    "bornacnn.fit(X,Y)# Fit does not train the Kernels yet. This is the next step. \n",
    "%matplotlib inline  \n",
    "plt.plot(range(len(bornacnn.errors)),bornacnn.errors)\n",
    "plt.xlabel('Epoc')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
