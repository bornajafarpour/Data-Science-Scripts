{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own BackPropagation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#let's define some basic functions. Even though I am not using anything besides sigmoid function\n",
    "#, I have defined the softmax and the derivative \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(np.zeros(x.shape[0]),x)\n",
    "\n",
    "def identity_derivative(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def sigmoid_derivative(sigmoid_x): # we calculate the derivative based on the sigmoid function value\n",
    "    return sigmoid_x*(1-sigmoid_x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1-x**2\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid:\n",
    "        return sigmoid_derivative\n",
    "    elif f == np.tanh:\n",
    "        return tanh_derivative\n",
    "    elif f == identity:\n",
    "        return identity_derivative\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def maxpool(x,pool_d):\n",
    "    \n",
    "    def local_max_indices(x,pool_d): #if is not devidable wihch can happen as a result of conv, we need to do something about it\n",
    "        \"\"\"Return maximum in groups of pool_dxpool_d for a N,h,w image\"\"\"\n",
    "        N,h,w = x.shape\n",
    "        x = x.reshape(N,h/pool_d,pool_d,w/pool_d,pool_d).swapaxes(2,3).reshape(N,h/pool_d,w/pool_d,pool_d*pool_d)\n",
    "        return np.argmax(x,axis=3)    \n",
    "    \n",
    "    def global_max_indices(x,pool_d):\n",
    "        N = x.shape[0]\n",
    "        image_d= x.shape[1]\n",
    "        ip_ratio = image_d / pool_d\n",
    "        lmi = local_max_indices(x,pool_d)\n",
    "        max_local_x,max_local_y = np.unravel_index(lmi.flat,dims=(pool_d,pool_d))\n",
    "        max_y =  max_local_y + np.tile(np.tile(range(ip_ratio),ip_ratio)*pool_d,N)\n",
    "        max_x =  max_local_x + np.tile(np.repeat(np.arange(ip_ratio), ip_ratio)*pool_d,N)\n",
    "        Ns = np.repeat(np.arange(N),ip_ratio**2)\n",
    "        return np.vstack([Ns,max_x,max_y]).T    \n",
    "    \n",
    "    N= x.shape[0]\n",
    "    image_d = x.shape[1]\n",
    "    crop_length = image_d%pool_d\n",
    "    x = x[:,:image_d-crop_length,:image_d-crop_length]\n",
    "    gmi = global_max_indices(x,pool_d)\n",
    "    maxes =  x[gmi[:,0],gmi[:,1],gmi[:,2]].reshape(N,image_d/pool_d,image_d/pool_d)\n",
    "    return gmi,maxes\n",
    "\n",
    "def up_sample(gmi,values,image_d,pool_d):\n",
    "    N =values.shape[0] \n",
    "    out = np.zeros([N,image_d,image_d])\n",
    "    out[gmi[:,0],gmi[:,1],gmi[:,2]] = 1 #maxes are equal to one\n",
    "    val_repeated = np.repeat(np.repeat(values,pool_d,axis=1),pool_d,axis=2)\n",
    "    crop_length = image_d - val_repeated.shape[1]\n",
    "    val_repeated = np.pad(val_repeated, ((0,0),(0,crop_length), (0,crop_length)), mode='constant', constant_values=0) # pad with zero to reverse cropping \n",
    "    return out * val_repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My backpropagation for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "class bornaCNN:\n",
    "    def __init__(self, mlp_layers_sizes, mlp_activations, CNN_activations,input_dimension ,num_of_kernels=5\n",
    "                 ,kernels_dimensions=[5], max_pool_dimensions = [2] , eta=0.05, epocs=5, update='online', verbose=True):\n",
    "        \n",
    "        self.errors=[]\n",
    "        self.eta=eta\n",
    "\n",
    "        self.epocs=epocs\n",
    "        self.num_of_kernels = num_of_kernels        \n",
    "        self.kernels_dimensions = kernels_dimensions\n",
    "        self.mlp_activations,self.CNN_activations = mlp_activations,CNN_activations\n",
    "        self.max_pool_dimensions = max_pool_dimensions\n",
    "        ######## mlp_layers_sizes doesn't contain the the number of nodes between the last maxp\n",
    "        # we calculate this->\n",
    "        \n",
    "        lcmld = input_dimension # lcmd = Last Conv-Max Layer Dimension\n",
    "        for i in range(len(kernels_dimensions)):\n",
    "            lcmld = lcmld - kernels_dimensions[i]+1\n",
    "            lcmld = lcmld / max_pool_dimensions[i]\n",
    "\n",
    "        self.mlp_layers_sizes = np.insert(mlp_layers_sizes,0,num_of_kernels*lcmld*lcmld) # does not contain the \n",
    "        #####################\n",
    "        self.num_mlp_layers = len(self.mlp_layers_sizes )\n",
    "        self.verbose= verbose\n",
    "        self.kernels_W , self.kernels_B = self.generate_random_kernels()\n",
    "        #self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.maxp_gmi = []# global max index of the maxpool layer. we use it to back propagate the deltas backward\n",
    "        self.num_of_kernels = num_of_kernels\n",
    "    \n",
    "    \n",
    "    def generate_random_kernels(self):#for now, we just use the same initialization technic that we use for MLP weights\n",
    "        kernels_W={}\n",
    "        kernels_B={}\n",
    "        for i,kd in enumerate(self.kernels_dimensions):# number of Convolutions layers\n",
    "            i = 2*i\n",
    "            #upper = 4*sqrt(6)/sqrt(self.layers_sizes[i]+self.layers_sizes[i+1])\n",
    "            upper = 4*sqrt(6)/sqrt(kd**2+1)\n",
    "            key = str(i)+'-'+str(i+1)\n",
    "            kernels_B[key] = np.zeros(self.num_of_kernels)\n",
    "            #kernels_W[key] = np.random.uniform(-upper,upper,[self.num_of_kernels,kd,kd])\n",
    "            kernels_W[key] = np.random.uniform(-3.,3.,[self.num_of_kernels,kd,kd])\n",
    "        return kernels_W,kernels_B\n",
    "        \n",
    "    def generate_random_mlp_weights(self):\n",
    "        mlp_W={}\n",
    "        mlp_B={}\n",
    "        for i in range(len(self.mlp_layers_sizes)-1):\n",
    "            upper = 4*sqrt(6)/sqrt(self.mlp_layers_sizes[i]+self.mlp_layers_sizes[i+1])\n",
    "            #weight also includes biases\n",
    "            mlp_B[str(i)+\"-\"+str(i+1)] = np.zeros(self.mlp_layers_sizes[i+1])\n",
    "            mlp_W[str(i)+\"-\"+str(i+1)] = np.random.uniform(-upper,upper, self.mlp_layers_sizes[i:i+2])#np.ones( self.layers_sizes[i:i+2])\n",
    "        return mlp_W,mlp_B\n",
    "    \n",
    "    def _calc_error(self,X,Y):\n",
    "        prediction = np.zeros(Y.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            ffr_K,gmi,ffr_mlp = self.feed_forward(X[i])\n",
    "            prediction[i] = ffr_mlp[-1]\n",
    "        error = .5*np.sum((Y-prediction)**2)/(X.shape[0])\n",
    "        print \"Error->\",error\n",
    "        return error\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        self.errors=[]\n",
    "        self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.errors.append(self._calc_error(X,Y))# Error before we start training        \n",
    "        for i in range(self.epocs):\n",
    "            if self.verbose:\n",
    "                if i<10:\n",
    "                    print (\"epoc->\",i+1)\n",
    "                elif i<100 and i%10==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<1000 and i%100==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<10000 and i%1000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<100000 and i%10000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "            \n",
    "            for j in range(X.shape[0]):\n",
    "                inputx = X[j,:,:]\n",
    "                target = Y[j,:]\n",
    "                ffr_K,gmi,ffr_mlp = self.feed_forward(inputx)\n",
    "                first_delta, delta_MLP_W, delta_MLP_B = self.calc_weight_updates_MLP(ffr_mlp,target)\n",
    "                #delta_K_W, delta_K_B = calc_weight_updates_K(ffr_K , gmi , first_delta)\n",
    "                self.update_MLP_weights(delta_MLP_W, delta_MLP_B)\n",
    "                \n",
    "            self.errors.append(self._calc_error(X,Y))\n",
    "                \n",
    "    def update_MLP_weights(self,delta_W,delta_B):\n",
    "        for i in range(self.num_mlp_layers-1): #going through layers\n",
    "            index=str(i)+\"-\"+str(i+1)\n",
    "            self.mlp_W[index] -= delta_W[index]\n",
    "            self.mlp_B[index] -= delta_B[index]\n",
    "\n",
    "#     def predict(self,X):\n",
    "#         nn_output = self.feed_forward(X)\n",
    "#         labels = np.argmax(nn_output[-1],axis=1)\n",
    "#         return labels\n",
    "    \n",
    "    def calc_weight_updates_MLP(self,ffr_mlp,target):\n",
    "        # https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/        \n",
    "        #step1: output layer\n",
    "        delta_W={}\n",
    "        delta_B={}\n",
    "        \n",
    "        this_output = ffr_mlp[-1]\n",
    "        previous_output = ffr_mlp[-2]\n",
    "\n",
    "        act_deriv = derivative(self.mlp_activations[self.num_mlp_layers-2])\n",
    "        this_delta = ((this_output - target)) * act_deriv(this_output) \n",
    "\n",
    "        weight_chagnge = np.outer(previous_output , this_delta)\n",
    "        \n",
    "        delta_W[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = self.eta * weight_chagnge\n",
    "        delta_B[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = self.eta *this_delta\n",
    "        \n",
    "        #step2: Hidden Layers\n",
    "        for i in reversed(range(0,self.num_mlp_layers-1)): # Going through all the layers backwards* changed range(1 to range(0\n",
    "            next_layer_delta = this_delta\n",
    "            hl_out_weights = self.mlp_W[str(i)+\"-\"+str(i+1)]            \n",
    "            \n",
    "            if i>0:\n",
    "                hl_input = ffr_mlp[i-1]\n",
    "                hl_output = ffr_mlp[i]\n",
    "                act_deriv = derivative(self.mlp_activations[i-1])\n",
    "                this_delta = np.dot(hl_out_weights, next_layer_delta) * act_deriv(hl_output)\n",
    "                weight_chagnge = np.outer(hl_input,this_delta)\n",
    "                delta_W[str(i-1)+\"-\"+str(i)] = self.eta* weight_chagnge     \n",
    "                delta_B[str(i-1)+\"-\"+str(i)] = self.eta* this_delta\n",
    "            else:#i ==0 input layer's output is f(x)=x\n",
    "                hl_out_weights = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "                first_delta = np.dot(hl_out_weights, next_layer_delta) # this part is equal to 1* act_deriv(hl_output)\n",
    "            \n",
    "        return first_delta, delta_W, delta_B\n",
    "    \n",
    "    def feed_forward(self,x): #feed forward, x is a 2d image matrix\n",
    "        x = np.array(x)        \n",
    "        ffr_K = []\n",
    "        layer_input = np.repeat(x[np.newaxis],self.num_of_kernels,axis=0)# make 2d inpu to k, 2d inpus\n",
    "        \n",
    "        for i,kd in enumerate(self.kernels_dimensions): #going through each Con layer and the max pool layer\n",
    "            \n",
    "            # We do apply the kernels first\n",
    "            conv_layer_output = np.zeros([self.num_of_kernels,layer_input.shape[1]-kd+1,layer_input.shape[1]-kd+1],dtype=float) # conv_output empty\n",
    "            key = str(2*i)+'-'+str(2*i+1)\n",
    "            Ks,Bs = self.kernels_W[key] , self.kernels_B[key]\n",
    "            for j in range(Ks.shape[0]):#iterating through the each kernel\n",
    "                conv_layer_output[j] = convolve2d(layer_input[j],Ks[j],'valid') + Bs[j]\n",
    "            conv_layer_output = self.CNN_activations[i](conv_layer_output) #applying the activation function\n",
    "            ffr_K.append(conv_layer_output) #append the convolution\n",
    "            ## Max pool dimension\n",
    "            maxp_d = self.max_pool_dimensions[i]\n",
    "            gmi, maxp_output = maxpool(conv_layer_output,maxp_d)\n",
    "            self.maxp_gmi.append(gmi)\n",
    "            ffr_K.append(maxp_output)\n",
    "            layer_input = maxp_output # for the next iteration\n",
    "\n",
    "        # before passing on the last layer to the fully connect network, we need to flatten it: layer_input.reshape(-1)\n",
    "        #layer_input is actually the output of last layer before MLP\n",
    "        \n",
    "        x = layer_input.reshape(-1)#flatten the 2d maxpool output\n",
    "        ffr_mlp=[x]#Feed Forward Result\n",
    "        for i in range(self.num_mlp_layers-1):\n",
    "            W = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "            B = self.mlp_B[str(i)+\"-\"+str(i+1)]\n",
    "            y = self.mlp_activations[i](np.dot(x,W) + B) #clculate the output of the layer\n",
    "            x=y\n",
    "            ffr_mlp.append(y)\n",
    "        return ffr_K,gmi,ffr_mlp\n",
    "    def calc_weight_updates_K(self,ffr_K,gmi,first_delta):    \n",
    "        next_delta = first_delta\n",
    "        for i in reversed(range(len(self.kernels_dimensions))):\n",
    "            #maxpool delta\n",
    "            maxpool_delta = up_sample(gmi[i], next_delta , image_d,self.max_pool_dimensions[i]):\n",
    "            #kernel delta\n",
    "            #kernel weight delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error-> 0.689949646021\n",
      "('epoc->', 1)\n",
      "Error-> 0.342615610235\n",
      "('epoc->', 2)\n",
      "Error-> 0.257185763188\n",
      "('epoc->', 3)\n",
      "Error-> 0.225035867756\n",
      "('epoc->', 4)\n",
      "Error-> 0.205712753992\n",
      "('epoc->', 5)\n",
      "Error-> 0.190301558477\n",
      "('epoc->', 6)\n",
      "Error-> 0.176986749242\n",
      "('epoc->', 7)\n",
      "Error-> 0.168171110741\n",
      "('epoc->', 8)\n",
      "Error-> 0.155595182852\n",
      "('epoc->', 9)\n",
      "Error-> 0.136191205588\n",
      "('epoc->', 10)\n",
      "Error-> 0.123383118075\n"
     ]
    }
   ],
   "source": [
    "bornacnn = bornaCNN(mlp_layers_sizes=[90,10], mlp_activations=[sigmoid,sigmoid,sigmoid,sigmoid,sigmoid], CNN_activations = [sigmoid,sigmoid],input_dimension = 28 ,\n",
    "                    num_of_kernels=4,kernels_dimensions=[4], max_pool_dimensions = [2],epocs=10, eta = 0.1)\n",
    "bornacnn.fit(X,Y)# Fit does not train the Kernels yet. This is the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_images = pd.read_csv(\"./train.csv\")\n",
    "train_images = train_images[:1000]\n",
    "\n",
    "# test_images = pd.read_csv(\"./test.csv\")\n",
    "#train_images = train_images.loc[1:1000]\n",
    "\n",
    "train_images_numpy = train_images[train_images.columns[1:]].as_matrix().astype(float)\n",
    "from sklearn.preprocessing import scale,LabelBinarizer\n",
    "train_scaled  = scale(train_images_numpy)\n",
    "\n",
    "X = train_scaled.reshape(-1,28,28)# making the images square before feeding it to the CNN\n",
    "lb = LabelBinarizer()\n",
    "Y= lb.fit_transform(train_images['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's see if the MLP part of the CNN is trained properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE4NBktTEYJAkNTEYJElN/j+WrfjH5JZN0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19e2aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(range(len(bornacnn.errors)),bornacnn.errors)\n",
    "plt.xlabel('Epoc')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The figure above shows that without Kernel learning, we acheive almost zero training error. This shows me that the MLP backpropagation and my feed forward are fine. The next step is to implement my Kernel learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ffr_K,gmi,ffr_mlp = bornacnn.feed_forward(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576L, 3L)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
