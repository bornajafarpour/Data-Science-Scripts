{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own BackPropagation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pool_d = 3\n",
    "# image_d = 7\n",
    "# x = np.random.randint(10,size=(2,image_d,image_d))\n",
    "# print x\n",
    "# gmi,maxes = maxpool(x,pool_d)\n",
    "# up_sample(gmi,maxes,image_d,pool_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#let's define some basic functions. Even though I am not using anything besides sigmoid function\n",
    "#, I have defined the softmax and the derivative \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(np.zeros(x.shape[0]),x)\n",
    "\n",
    "def identity_derivative(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def sigmoid_derivative(sigmoid_x): # we calculate the derivative based on the sigmoid function value\n",
    "    return sigmoid_x*(1-sigmoid_x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1-x**2\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid:\n",
    "        return sigmoid_derivative\n",
    "    elif f == np.tanh:\n",
    "        return tanh_derivative\n",
    "    elif f == identity:\n",
    "        return identity_derivative\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def maxpool(x,pool_d):\n",
    "    \n",
    "    def local_max_indices(x,pool_d): #if is not devidable wihch can happen as a result of conv, we need to do something about it\n",
    "        \"\"\"Return maximum in groups of pool_dxpool_d for a N,h,w image\"\"\"\n",
    "        N,h,w = x.shape\n",
    "        x = x.reshape(N,h/pool_d,pool_d,w/pool_d,pool_d).swapaxes(2,3).reshape(N,h/pool_d,w/pool_d,pool_d*pool_d)\n",
    "        return np.argmax(x,axis=3)    \n",
    "    \n",
    "    def global_max_indices(x,pool_d):\n",
    "        N = x.shape[0]\n",
    "        image_d= x.shape[1]\n",
    "        ip_ratio = image_d / pool_d\n",
    "        lmi = local_max_indices(x,pool_d)\n",
    "        max_local_x,max_local_y = np.unravel_index(lmi.flat,dims=(pool_d,pool_d))\n",
    "        max_y =  max_local_y + np.tile(np.tile(range(ip_ratio),ip_ratio)*pool_d,N)\n",
    "        max_x =  max_local_x + np.tile(np.repeat(np.arange(ip_ratio), ip_ratio)*pool_d,N)\n",
    "        Ns = np.repeat(np.arange(N),ip_ratio**2)\n",
    "        return np.vstack([Ns,max_x,max_y]).T    \n",
    "    \n",
    "    N= x.shape[0]\n",
    "    image_d = x.shape[1]\n",
    "    crop_length = image_d%pool_d\n",
    "    x = x[:,:image_d-crop_length,:image_d-crop_length]\n",
    "    gmi = global_max_indices(x,pool_d)\n",
    "    maxes =  x[gmi[:,0],gmi[:,1],gmi[:,2]].reshape(N,image_d/pool_d,image_d/pool_d)\n",
    "    return gmi,maxes\n",
    "\n",
    "def up_sample(gmi,values,image_d,pool_d):\n",
    "    N =values.shape[0] \n",
    "    out = np.zeros([N,image_d,image_d])\n",
    "    out[gmi[:,0],gmi[:,1],gmi[:,2]] = 1 #maxes are equal to one\n",
    "    val_repeated = np.repeat(np.repeat(values,pool_d,axis=1),pool_d,axis=2)\n",
    "    crop_length = image_d - val_repeated.shape[1]\n",
    "    val_repeated = np.pad(val_repeated, ((0,0),(0,crop_length), (0,crop_length)), mode='constant', constant_values=0) # pad with zero to reverse cropping \n",
    "    return out * val_repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My backpropagation for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "class bornaCNN:\n",
    "    def __init__(self, mlp_layers_sizes, mlp_activations, CNN_activations,input_dimension ,num_of_kernels=5\n",
    "                 ,kernels_dimensions=[5], max_pool_dimensions = [2] , eta=0.05, epocs=5, update='online', verbose=True):\n",
    "        \n",
    "        self.errors=[]\n",
    "        self.eta=eta\n",
    "\n",
    "        self.epocs=epocs\n",
    "        self.num_of_kernels = num_of_kernels        \n",
    "        self.kernels_dimensions = kernels_dimensions\n",
    "        self.mlp_activations,self.CNN_activations = mlp_activations,CNN_activations\n",
    "        self.max_pool_dimensions = max_pool_dimensions\n",
    "        ######## mlp_layers_sizes doesn't contain the the number of nodes between the last maxp\n",
    "        # we calculate this->\n",
    "        \n",
    "        lcmld = input_dimension # lcmd = Last Conv-Max Layer Dimension\n",
    "        for i in range(len(kernels_dimensions)):\n",
    "            lcmld = lcmld - kernels_dimensions[i]+1\n",
    "            lcmld = lcmld / max_pool_dimensions[i]\n",
    "\n",
    "        self.mlp_layers_sizes = np.insert(mlp_layers_sizes,0,num_of_kernels*lcmld*lcmld) # does not contain the \n",
    "        #####################\n",
    "        self.num_mlp_layers = len(self.mlp_layers_sizes )\n",
    "        self.verbose= verbose\n",
    "        self.kernels_W , self.kernels_B = self.generate_random_kernels()\n",
    "        #self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.maxp_gmi = []# global max index of the maxpool layer. we use it to back propagate the deltas backward\n",
    "        self.num_of_kernels = num_of_kernels\n",
    "    \n",
    "    \n",
    "    def generate_random_kernels(self):#for now, we just use the same initialization technic that we use for MLP weights\n",
    "        kernels_W={}\n",
    "        kernels_B={}\n",
    "        for i,kd in enumerate(self.kernels_dimensions):# number of Convolutions layers\n",
    "            i = 2*i\n",
    "            #upper = 4*sqrt(6)/sqrt(self.layers_sizes[i]+self.layers_sizes[i+1])\n",
    "            upper = 4*sqrt(6)/sqrt(kd**2+1)\n",
    "            key = str(i)+'-'+str(i+1)\n",
    "            kernels_B[key] = np.zeros(self.num_of_kernels)\n",
    "            #kernels_W[key] = np.random.uniform(-upper,upper,[self.num_of_kernels,kd,kd])\n",
    "            kernels_W[key] = np.random.uniform(-3.,3.,[self.num_of_kernels,kd,kd])\n",
    "        return kernels_W,kernels_B\n",
    "        \n",
    "    def generate_random_mlp_weights(self):\n",
    "        mlp_W={}\n",
    "        mlp_B={}\n",
    "        for i in range(len(self.mlp_layers_sizes)-1):\n",
    "            upper = 4*sqrt(6)/sqrt(self.mlp_layers_sizes[i]+self.mlp_layers_sizes[i+1])\n",
    "            #weight also includes biases\n",
    "            mlp_B[str(i)+\"-\"+str(i+1)] = np.zeros(self.mlp_layers_sizes[i+1])\n",
    "            mlp_W[str(i)+\"-\"+str(i+1)] = np.random.uniform(-upper,upper, self.mlp_layers_sizes[i:i+2])#np.ones( self.layers_sizes[i:i+2])\n",
    "        return mlp_W,mlp_B\n",
    "    \n",
    "    def _calc_error(self,X,Y):\n",
    "        prediction = np.zeros(Y.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            ffr_K,gmi,ffr_mlp = self.feed_forward(X[i])\n",
    "            prediction[i] = ffr_mlp[-1]\n",
    "        error = .5*np.sum((Y-prediction)**2)/(X.shape[0])\n",
    "        print \"Error->\",error\n",
    "        return error\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        self.errors=[]\n",
    "        self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.errors.append(self._calc_error(X,Y))# Error before we start training        \n",
    "        for i in range(self.epocs):\n",
    "            if self.verbose:\n",
    "                if i<10:\n",
    "                    print (\"epoc->\",i+1)\n",
    "                elif i<100 and i%10==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<1000 and i%100==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<10000 and i%1000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<100000 and i%10000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "            \n",
    "            for j in range(X.shape[0]):\n",
    "                inputx = X[j,:,:]\n",
    "                target = Y[j,:]\n",
    "                ffr_K,gmi,ffr_mlp = self.feed_forward(inputx)\n",
    "                delta_MLP_W, delta_MLP_B = self.calc_weight_updates_MLP(ffr_mlp,target)\n",
    "                #delta_K_W, delta_K_B = calc_weight_updates_K(ffr_K,gmi)\n",
    "                self.update_MLP_weights(delta_MLP_W, delta_MLP_B)\n",
    "                \n",
    "            self.errors.append(self._calc_error(X,Y))\n",
    "                \n",
    "    def update_MLP_weights(self,delta_W,delta_B):\n",
    "        for i in range(self.num_mlp_layers-1): #going through layers\n",
    "            index=str(i)+\"-\"+str(i+1)\n",
    "            self.mlp_W[index] -= delta_W[index]\n",
    "            self.mlp_B[index] -= delta_B[index]\n",
    "\n",
    "    def predict(self,X):\n",
    "        nn_output = self.feed_forward(X)\n",
    "        labels = np.argmax(nn_output[-1],axis=1)\n",
    "        return labels\n",
    "    \n",
    "    def calc_weight_updates_MLP(self,ffr_mlp,target):\n",
    "        # https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/        \n",
    "        #step1: output layer\n",
    "        delta_W={}\n",
    "        delta_B={}\n",
    "        \n",
    "        this_output = ffr_mlp[-1]\n",
    "        previous_output = ffr_mlp[-2]\n",
    "\n",
    "        act_deriv = derivative(self.mlp_activations[self.num_mlp_layers-2])\n",
    "        this_delta = ((this_output - target)) * act_deriv(this_output) \n",
    "\n",
    "        weight_chagnge = np.outer(previous_output , this_delta)\n",
    "        \n",
    "        delta_W[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = self.eta * weight_chagnge\n",
    "        delta_B[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = self.eta *this_delta\n",
    "        \n",
    "        #step2: Hidden Layers\n",
    "        for i in reversed(range(1,self.num_mlp_layers-1)): # Going through all the layers backwards* changed range(1 to range(0\n",
    "            next_layer_delta = this_delta\n",
    "            \n",
    "            hl_input = ffr_mlp[i-1]\n",
    "            hl_output = ffr_mlp[i]\n",
    "            hl_out_weights = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "            \n",
    "            act_deriv = derivative(self.mlp_activations[i-1])\n",
    "            this_delta =np.dot(hl_out_weights, next_layer_delta) * act_deriv(hl_output)\n",
    "            \n",
    "            weight_chagnge = np.outer(hl_input,this_delta)\n",
    "            delta_W[str(i-1)+\"-\"+str(i)] = self.eta* weight_chagnge     \n",
    "            delta_B[str(i-1)+\"-\"+str(i)] = self.eta* this_delta\n",
    "            \n",
    "        # step3 iterating betwen Maxpool and conv steps and calculating the weight updates\n",
    "            \n",
    "        return delta_W, delta_B\n",
    "    \n",
    "    def feed_forward(self,x): #feed forward, x is a 2d matrix\n",
    "        x = np.array(x)        \n",
    "        ffr_K = []\n",
    "        layer_input = np.repeat(x[np.newaxis],self.num_of_kernels,axis=0)# make 2d inpu to k, 2d inpus\n",
    "        \n",
    "        for i,kd in enumerate(self.kernels_dimensions): #going through each Con layer and the max pool layer\n",
    "            \n",
    "            # We do apply the kernels first\n",
    "            conv_layer_output = np.zeros([self.num_of_kernels,layer_input.shape[1]-kd+1,layer_input.shape[1]-kd+1],dtype=float) # conv_output empty\n",
    "            key = str(2*i)+'-'+str(2*i+1)\n",
    "            Ks,Bs = self.kernels_W[key] , self.kernels_B[key]\n",
    "            for j in range(Ks.shape[0]):#iterating through the each kernel\n",
    "                conv_layer_output[j] = convolve2d(layer_input[j],Ks[j],'valid') + Bs[j]\n",
    "            conv_layer_output = self.CNN_activations[i](conv_layer_output) #applying the activation function\n",
    "            ffr_K.append(conv_layer_output) #append the convolution\n",
    "            ## Max pool dimension\n",
    "            maxp_d = self.max_pool_dimensions[i]\n",
    "            gmi, maxp_output = maxpool(conv_layer_output,maxp_d)\n",
    "            self.maxp_gmi.append(gmi)\n",
    "            ffr_K.append(maxp_output)\n",
    "            layer_input = maxp_output # for the next iteration\n",
    "\n",
    "        # before passing on the last layer to the fully connect network, we need to flatten it: layer_input.reshape(-1)\n",
    "        #layer_input is actually the output of last layer before MLP\n",
    "        \n",
    "        x = layer_input.reshape(-1)#flatten the 2d maxpool output\n",
    "        ffr_mlp=[x]#Feed Forward Result\n",
    "        for i in range(self.num_mlp_layers-1):\n",
    "            W = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "            B = self.mlp_B[str(i)+\"-\"+str(i+1)]\n",
    "            y = self.mlp_activations[i](np.dot(x,W) + B) #clculate the output of the layer\n",
    "            x=y\n",
    "            ffr_mlp.append(y)\n",
    "        return ffr_K,gmi,ffr_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_images = pd.read_csv(\"./train.csv\")\n",
    "train_images = train_images[:1000]\n",
    "\n",
    "# test_images = pd.read_csv(\"./test.csv\")\n",
    "#train_images = train_images.loc[1:1000]\n",
    "\n",
    "\n",
    "train_images_numpy = train_images[train_images.columns[1:]].as_matrix().astype(float)\n",
    "from sklearn.preprocessing import scale,LabelBinarizer\n",
    "train_scaled  = scale(train_images_numpy)\n",
    "\n",
    "X = train_scaled.reshape(-1,28,28)# making the images square before feeding it to the \n",
    "lb = LabelBinarizer()\n",
    "Y= lb.fit_transform(train_images['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error-> 2.35829215938\n",
      "('epoc->', 1)\n",
      "Error-> 0.748650618335\n",
      "('epoc->', 2)\n",
      "Error-> 0.302608761423\n",
      "('epoc->', 3)\n",
      "Error-> 0.18296839782\n",
      "('epoc->', 4)\n",
      "Error-> 0.147557164193\n",
      "('epoc->', 5)\n",
      "Error-> 0.127514083104\n",
      "('epoc->', 6)\n",
      "Error-> 0.114247101702\n",
      "('epoc->', 7)\n",
      "Error-> 0.104170185013\n",
      "('epoc->', 8)\n",
      "Error-> 0.09577204838\n",
      "('epoc->', 9)\n",
      "Error-> 0.0885015591578\n",
      "('epoc->', 10)\n",
      "Error-> 0.0821035956298\n",
      "('epoc->', 10)\n",
      "Error-> 0.0762275244934\n",
      "Error-> 0.0708857417824\n",
      "Error-> 0.066276214873\n",
      "Error-> 0.0621341522841\n"
     ]
    }
   ],
   "source": [
    "bornacnn = bornaCNN(mlp_layers_sizes=[60,10], mlp_activations=[sigmoid,sigmoid,sigmoid,sigmoid,sigmoid], CNN_activations = [sigmoid,sigmoid],input_dimension = 28 ,\n",
    "                    num_of_kernels=4,kernels_dimensions=[4], max_pool_dimensions = [2],epocs=20, eta = 0.05)\n",
    "bornacnn.fit(X,Y)\n",
    "print bornacnn.errors\n",
    "plt.plot(range(len(bornacnn.errors)),bornacnn.errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
