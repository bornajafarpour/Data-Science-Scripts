{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own BackPropagation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#let's define some basic functions. Even though I am not using anything besides sigmoid function\n",
    "#, I have defined the softmax and the derivative \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(np.zeros(x.shape[0]),x)\n",
    "\n",
    "def identity_derivative(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def sigmoid_derivative(sigmoid_x): # we calculate the derivative based on the sigmoid function value\n",
    "    return sigmoid_x*(1-sigmoid_x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1-x**2\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid:\n",
    "        return sigmoid_derivative\n",
    "    elif f == np.tanh:\n",
    "        return tanh_derivative\n",
    "    elif f == identity:\n",
    "        return identity_derivative\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "rot180 = lambda X: np.rot90(X,2)\n",
    "    \n",
    "def maxpool(x,pool_d):\n",
    "    \n",
    "    def local_max_indices(x,pool_d): #if is not devidable wihch can happen as a result of conv, we need to do something about it\n",
    "        \"\"\"Return maximum in groups of pool_dxpool_d for a N,h,w image\"\"\"\n",
    "        N,h,w = x.shape\n",
    "        x = x.reshape(N,h/pool_d,pool_d,w/pool_d,pool_d).swapaxes(2,3).reshape(N,h/pool_d,w/pool_d,pool_d*pool_d)\n",
    "        return np.argmax(x,axis=3)    \n",
    "    \n",
    "    def global_max_indices(x,pool_d):\n",
    "        N = x.shape[0]\n",
    "        image_d= x.shape[1]\n",
    "        ip_ratio = image_d / pool_d\n",
    "        lmi = local_max_indices(x,pool_d)\n",
    "        max_local_x,max_local_y = np.unravel_index(lmi.flat,dims=(pool_d,pool_d))\n",
    "        max_y =  max_local_y + np.tile(np.tile(range(ip_ratio),ip_ratio)*pool_d,N)\n",
    "        max_x =  max_local_x + np.tile(np.repeat(np.arange(ip_ratio), ip_ratio)*pool_d,N)\n",
    "        Ns = np.repeat(np.arange(N),ip_ratio**2)\n",
    "        return np.vstack([Ns,max_x,max_y]).T    \n",
    "    \n",
    "    N= x.shape[0]\n",
    "    image_d = x.shape[1]\n",
    "    crop_length = image_d%pool_d\n",
    "    x = x[:,:image_d-crop_length,:image_d-crop_length]\n",
    "    gmi = global_max_indices(x,pool_d)\n",
    "    maxes =  x[gmi[:,0],gmi[:,1],gmi[:,2]].reshape(N,image_d/pool_d,image_d/pool_d)\n",
    "    return gmi,maxes\n",
    "\n",
    "def up_sample(gmi,values,image_d,pool_d):\n",
    "    N =values.shape[0] \n",
    "    out = np.zeros([N,image_d,image_d])\n",
    "    out[gmi[:,0],gmi[:,1],gmi[:,2]] = 1 #maxes are equal to one\n",
    "    val_repeated = np.repeat(np.repeat(values,pool_d,axis=1),pool_d,axis=2)\n",
    "    crop_length = image_d - val_repeated.shape[1]\n",
    "    val_repeated = np.pad(val_repeated, ((0,0),(0,crop_length), (0,crop_length)), mode='constant', constant_values=0) # pad with zero to reverse cropping \n",
    "    return out * val_repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My backpropagation for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "class bornaCNN:\n",
    "    def __init__(self, mlp_layers_sizes, mlp_activations, CNN_activations,input_dimension ,num_of_kernels=5\n",
    "                 ,kernels_dimensions=[5], max_pool_dimensions = [2] , eta=0.05, epocs=5, update='online', verbose=True):\n",
    "        \n",
    "        self.errors=[]\n",
    "        self.eta=eta\n",
    "\n",
    "        self.epocs=epocs\n",
    "        self.num_of_kernels = num_of_kernels        \n",
    "        self.kernels_dimensions = kernels_dimensions\n",
    "        self.mlp_activations,self.CNN_activations = mlp_activations,CNN_activations\n",
    "        self.max_pool_dimensions = max_pool_dimensions\n",
    "        ######## mlp_layers_sizes doesn't contain the the number of nodes between the last maxp\n",
    "        # we calculate this->\n",
    "        \n",
    "        lcmld = input_dimension # lcmd = Last Conv-Max Layer Dimension\n",
    "        for i in range(len(kernels_dimensions)):\n",
    "            lcmld = lcmld - kernels_dimensions[i]+1\n",
    "            lcmld = lcmld / max_pool_dimensions[i]\n",
    "\n",
    "        self.mlp_layers_sizes = np.insert(mlp_layers_sizes,0,num_of_kernels*lcmld*lcmld) # does not contain the \n",
    "        #####################\n",
    "        self.num_mlp_layers = len(self.mlp_layers_sizes )\n",
    "        self.verbose= verbose\n",
    "        self.kernels_W , self.kernels_B = self.generate_random_kernels()\n",
    "        #self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.num_of_kernels = num_of_kernels\n",
    "    \n",
    "    \n",
    "    def generate_random_kernels(self):#for now, we just use the same initialization technic that we use for MLP weights\n",
    "        kernels_W={}\n",
    "        kernels_B={}\n",
    "        for i,kd in enumerate(self.kernels_dimensions):# number of Convolutions layers\n",
    "            i = 2*i\n",
    "            #upper = 4*sqrt(6)/sqrt(self.layers_sizes[i]+self.layers_sizes[i+1])\n",
    "            upper = 4*sqrt(6)/sqrt(kd**2+1)\n",
    "            key = str(i)+'-'+str(i+1)\n",
    "            kernels_B[key] = np.zeros(self.num_of_kernels)\n",
    "            #kernels_W[key] = np.random.uniform(-upper,upper,[self.num_of_kernels,kd,kd])\n",
    "            kernels_W[key] = np.random.uniform(-3.,3.,[self.num_of_kernels,kd,kd])\n",
    "        return kernels_W,kernels_B\n",
    "        \n",
    "    def generate_random_mlp_weights(self):\n",
    "        mlp_W={}\n",
    "        mlp_B={}\n",
    "        for i in range(len(self.mlp_layers_sizes)-1):\n",
    "            upper = 4*sqrt(6)/sqrt(self.mlp_layers_sizes[i]+self.mlp_layers_sizes[i+1])\n",
    "            #weight also includes biases\n",
    "            mlp_B[str(i)+\"-\"+str(i+1)] = np.zeros(self.mlp_layers_sizes[i+1])\n",
    "            mlp_W[str(i)+\"-\"+str(i+1)] = np.random.uniform(-upper,upper, self.mlp_layers_sizes[i:i+2])#np.ones( self.layers_sizes[i:i+2])\n",
    "        return mlp_W,mlp_B\n",
    "    \n",
    "    def _calc_error(self,X,Y):\n",
    "        prediction = np.zeros(Y.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            ffr_K,gmi,ffr_mlp = self.feed_forward(X[i])\n",
    "            prediction[i] = ffr_mlp[-1]\n",
    "        error = .5*np.sum((Y-prediction)**2)/(X.shape[0])\n",
    "        print \"Error->\",error\n",
    "        return error\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        self.errors=[]\n",
    "        self.mlp_W , self.mlp_B= self.generate_random_mlp_weights()\n",
    "        self.errors.append(self._calc_error(X,Y))# Error before we start training        \n",
    "        for i in range(self.epocs):\n",
    "            if self.verbose:\n",
    "                if i<10:\n",
    "                    print (\"epoc->\",i+1)\n",
    "                elif i<100 and i%10==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<1000 and i%100==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<10000 and i%1000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "                elif i<100000 and i%10000==0:\n",
    "                    print (\"epoc->\",i)\n",
    "            \n",
    "            for j in range(X.shape[0]):\n",
    "                inputx = X[j,:,:]\n",
    "                target = Y[j,:]\n",
    "                ffr_K,all_gmi,ffr_mlp = self.feed_forward(inputx)\n",
    "                \n",
    "                first_delta, delta_MLP_W, delta_MLP_B = self.calc_weight_updates_MLP(ffr_mlp,target)\n",
    "                self.calc_weight_updates_K(ffr_K , all_gmi , first_delta)\n",
    "                self.update_MLP_weights(delta_MLP_W, delta_MLP_B)\n",
    "                \n",
    "            self.errors.append(self._calc_error(X,Y))\n",
    "                \n",
    "    def update_weights(self,delta_W,delta_B,layer_type):\n",
    "        \n",
    "        if layer_type.lower() == 'mlp':\n",
    "            for i in range(self.num_mlp_layers-1): #going through layers\n",
    "                index=str(i)+\"-\"+str(i+1)\n",
    "                self.mlp_W[index] -= delta_W[index]\n",
    "                self.mlp_B[index] -= delta_B[index]\n",
    "        else: #conv\n",
    "            for key in delta_W\n",
    "                self.kernels_W[key] -= delta_W[key]\n",
    "                self.kernels_B[key] -= delta_B[key]\n",
    "\n",
    "#     def predict(self,X):\n",
    "#         nn_output = self.feed_forward(X)\n",
    "#         labels = np.argmax(nn_output[-1],axis=1)\n",
    "#         return labels\n",
    "    \n",
    "    def calc_weight_updates_MLP(self,ffr_mlp,target):\n",
    "        # https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/        \n",
    "        #step1: output layer\n",
    "        delta_W={}\n",
    "        delta_B={}\n",
    "        \n",
    "        this_output = ffr_mlp[-1]\n",
    "        previous_output = ffr_mlp[-2]\n",
    "\n",
    "        act_deriv = derivative(self.mlp_activations[self.num_mlp_layers-2])\n",
    "        this_delta = ((this_output - target)) * act_deriv(this_output) \n",
    "\n",
    "        weight_chagnge = np.outer(previous_output , this_delta)\n",
    "        \n",
    "        delta_W[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = self.eta * weight_chagnge\n",
    "        delta_B[str(self.num_mlp_layers-2)+\"-\"+str(self.num_mlp_layers-1)] = self.eta *this_delta\n",
    "        \n",
    "        #step2: Hidden Layers\n",
    "        for i in reversed(range(0,self.num_mlp_layers-1)): # Going through all the layers backwards* changed range(1 to range(0\n",
    "            next_layer_delta = this_delta\n",
    "            hl_out_weights = self.mlp_W[str(i)+\"-\"+str(i+1)]            \n",
    "            \n",
    "            if i>0:\n",
    "                hl_input = ffr_mlp[i-1]\n",
    "                hl_output = ffr_mlp[i]\n",
    "                act_deriv = derivative(self.mlp_activations[i-1])\n",
    "                this_delta = np.dot(hl_out_weights, next_layer_delta) * act_deriv(hl_output)\n",
    "                weight_chagnge = np.outer(hl_input,this_delta)\n",
    "                delta_W[str(i-1)+\"-\"+str(i)] = self.eta* weight_chagnge     \n",
    "                delta_B[str(i-1)+\"-\"+str(i)] = self.eta* this_delta\n",
    "            else:#i ==0 input layer's output is f(x)=x\n",
    "                hl_out_weights = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "                first_delta = np.dot(hl_out_weights, next_layer_delta) # this part is equal to 1* act_deriv(hl_output)\n",
    "            \n",
    "        return first_delta, delta_W, delta_B\n",
    "    \n",
    "    def feed_forward(self,x): #feed forward, x is a 2d image matrix\n",
    "        x = np.array(x)        \n",
    "        ffr_K = []\n",
    "        all_gmi=[]\n",
    "        layer_input = np.repeat(x[np.newaxis],self.num_of_kernels,axis=0)# make 2d inpu to k, 2d inpus\n",
    "        ffr_K.append(layer_input)\n",
    "        \n",
    "        for i,kd in enumerate(self.kernels_dimensions): #going through each Con layer and the max pool layer\n",
    "            \n",
    "            # We do apply the kernels first\n",
    "            conv_layer_output = np.zeros([self.num_of_kernels,layer_input.shape[1]-kd+1,layer_input.shape[1]-kd+1],dtype=float) # conv_output empty\n",
    "            key = str(2*i)+'-'+str(2*i+1)\n",
    "            Ks,Bs = self.kernels_W[key] , self.kernels_B[key]\n",
    "            for j in range(Ks.shape[0]):#iterating through the each kernel\n",
    "                conv_layer_output[j] = convolve2d(layer_input[j],Ks[j],'valid') + Bs[j]\n",
    "            conv_layer_output = self.CNN_activations[i](conv_layer_output) #applying the activation function\n",
    "            ffr_K.append(conv_layer_output) #append the convolution\n",
    "            ## Max pool dimension\n",
    "            maxp_d = self.max_pool_dimensions[i]\n",
    "            gmi, maxp_output = maxpool(conv_layer_output,maxp_d)\n",
    "            all_gmi.append(gmi)\n",
    "            ffr_K.append(maxp_output)\n",
    "            layer_input = maxp_output # for the next iteration\n",
    "\n",
    "        # before passing on the last layer to the fully connect network, we need to flatten it: layer_input.reshape(-1)\n",
    "        #layer_input is actually the output of last layer before MLP\n",
    "        \n",
    "        x = layer_input.reshape(-1)#flatten the 2d maxpool output\n",
    "        ffr_mlp=[x]#Feed Forward Result\n",
    "        for i in range(self.num_mlp_layers-1):\n",
    "            W = self.mlp_W[str(i)+\"-\"+str(i+1)]\n",
    "            B = self.mlp_B[str(i)+\"-\"+str(i+1)]\n",
    "            y = self.mlp_activations[i](np.dot(x,W) + B) #clculate the output of the layer\n",
    "            x=y\n",
    "            ffr_mlp.append(y)\n",
    "        return ffr_K,all_gmi,ffr_mlp\n",
    "    \n",
    "    def calc_weight_updates_K(self,ffr_K,gmi,first_delta):\n",
    "        detla_k_W={}\n",
    "        detla_k_B={}\n",
    "        delta_dim = int(sqrt(first_delta.shape[0] / self.num_of_kernels))\n",
    "       \n",
    "        maxpool_delta = first_delta.reshape(self.num_of_kernels,delta_dim,delta_dim)\n",
    "        for i in reversed(range(len(self.kernels_dimensions))):\n",
    "            #first the maxpool\n",
    "            image_d = ffr_K[i+1].shape[1]\n",
    "            conv_delta = up_sample(gmi[i],maxpool_delta ,image_d, self.max_pool_dimensions[i])\n",
    "            #conv layer delta\n",
    "            key = str(2*i)+'-'+str(2*i+1)\n",
    "            \n",
    "            \n",
    "            detla_k_W[key] = np.zeros(self.kernels_W[key].shape) \n",
    "            detla_k_B[key] = np.zeros(self.kernels_B[key].shape) \n",
    "            for j in range(self.num_of_kernels):\n",
    "                this_k = self.kernels_W[key][j]\n",
    "                detla_k_W[key][j] = rot180(convolve2d(ffr_K[2*i][j],rot180(conv_delta[j]),\"valid\"))\n",
    "                detla_k_B[key][j] = np.sum(conv_delta[j])\n",
    "        return detla_k_W,detla_k_B\n",
    "    \n",
    "    \n",
    "#                 print \"conv_delta[j]\",conv_delta[j].shape\n",
    "#                 print \"ffr_K[2*i][j]\",ffr_K[2*i][j].shape,i\n",
    "#                 print \"conv_delta[j].shape\",conv_delta[j].shape\n",
    "                #xyz= rot180(convolve2d(ffr_K[2*i][j],rot180(conv_delta[j]),\"valid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error-> 2.1802721464\n",
      "('epoc->', 1)\n",
      "Error-> 0.32607936149\n",
      "('epoc->', 2)\n",
      "Error-> 0.239820005404\n",
      "('epoc->', 3)\n",
      "Error-> 0.180348591082\n",
      "('epoc->', 4)\n",
      "Error-> 0.146410851574\n",
      "('epoc->', 5)\n",
      "Error-> 0.127736714373\n",
      "('epoc->', 6)\n",
      "Error-> 0.119049052093\n",
      "('epoc->', 7)\n",
      "Error-> 0.116339725408\n",
      "('epoc->', 8)\n",
      "Error-> 0.113749730934\n",
      "('epoc->', 9)\n",
      "Error-> 0.109644736427\n",
      "('epoc->', 10)\n",
      "Error-> 0.105138910431\n",
      "('epoc->', 10)\n",
      "Error-> 0.100993780336\n",
      "Error-> 0.0966113781224\n",
      "Error-> 0.0913763116573\n",
      "Error-> 0.0852579287219\n",
      "Error-> 0.0788108494752\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b5dcba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bornacnn = bornaCNN(mlp_layers_sizes=[50,10], \n",
    "                    mlp_activations=[sigmoid,sigmoid,sigmoid,sigmoid,sigmoid], \n",
    "                    CNN_activations = [sigmoid,sigmoid],input_dimension = 28 ,\n",
    "                    num_of_kernels=3,kernels_dimensions=[4], max_pool_dimensions = [2],\n",
    "                    epocs=15, eta = 0.1)\n",
    "bornacnn.fit(X,Y)# Fit does not train the Kernels yet. This is the next step. \n",
    "%matplotlib inline  \n",
    "plt.plot(range(len(bornacnn.errors)),bornacnn.errors)\n",
    "plt.xlabel('Epoc')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_images = pd.read_csv(\"./train.csv\")\n",
    "train_images = train_images[:1000]\n",
    "\n",
    "# test_images = pd.read_csv(\"./test.csv\")\n",
    "#train_images = train_images.loc[1:1000]\n",
    "\n",
    "train_images_numpy = train_images[train_images.columns[1:]].as_matrix().astype(float)\n",
    "from sklearn.preprocessing import scale,LabelBinarizer\n",
    "train_scaled  = scale(train_images_numpy)\n",
    "\n",
    "X = train_scaled.reshape(-1,28,28)# making the images square before feeding it to the CNN\n",
    "lb = LabelBinarizer()\n",
    "Y= lb.fit_transform(train_images['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's see if the MLP part of the CNN is trained properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The figure above shows that without Kernel learning, we acheive almost zero training error. This shows me that the MLP backpropagation and my feed forward are fine. The next step is to implement my Kernel learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ffr_K,gmi,ffr_mlp = bornacnn.feed_forward(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3L, 12L, 12L)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffr_K[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([[1,2,3],[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0-1': array([[-0.11120486,  0.10994512,  0.00810591, ..., -0.04228307,\n",
       "          0.13774619,  0.30441113],\n",
       "        [ 0.00108366,  0.07724026,  0.35866228, ...,  0.25012014,\n",
       "          0.03714752,  0.22241431],\n",
       "        [ 0.15738252, -0.13027819, -0.20468005, ...,  0.3811717 ,\n",
       "         -0.31438273, -0.20972829],\n",
       "        ..., \n",
       "        [-0.29562654,  0.40126997, -0.41558736, ..., -0.42419167,\n",
       "          0.1021486 ,  0.29826638],\n",
       "        [-0.01056214, -0.13175402,  0.13786924, ...,  0.00692824,\n",
       "          0.11839062,  0.04165835],\n",
       "        [ 0.06631433,  0.00581016, -0.221138  , ...,  0.40816699,\n",
       "         -0.17146952,  0.44203049]]),\n",
       " '1-2': array([[  9.05046798e-01,  -5.21395662e-02,  -1.27743816e-01,\n",
       "           9.26820059e-01,  -9.03883580e-01,   5.74406674e-01,\n",
       "          -4.10387128e-01,   1.06789293e-01,  -5.14941624e-01,\n",
       "           4.60353210e-01],\n",
       "        [ -1.19592851e+00,  -9.03950842e-01,  -1.02792273e+00,\n",
       "           2.48466533e-01,   3.15632964e-01,   2.10629309e-01,\n",
       "          -3.67913080e-01,  -3.69279735e-01,   3.87796489e-01,\n",
       "           8.87759348e-01],\n",
       "        [ -3.20587724e-01,   5.11131695e-02,  -1.75420938e-01,\n",
       "          -2.04175183e-01,   1.82978951e-01,   2.33920610e-01,\n",
       "           1.06962687e+00,  -4.16575197e-01,   6.81590713e-01,\n",
       "           9.38895316e-01],\n",
       "        [ -6.90627849e-01,   4.98917915e-02,   6.84618528e-01,\n",
       "          -5.53701597e-01,  -4.79714173e-01,  -1.61982593e-01,\n",
       "          -1.00938143e+00,  -5.98271320e-01,  -2.76611825e-01,\n",
       "          -3.39810414e-01],\n",
       "        [  5.90792399e-01,  -2.19659348e-01,  -7.20815716e-01,\n",
       "           5.31537502e-01,   2.63592070e-01,   5.00095624e-01,\n",
       "          -2.71257469e-01,   9.42707526e-01,  -6.08396354e-01,\n",
       "           1.11286009e+00],\n",
       "        [  5.73828576e-01,   6.07973064e-02,  -1.23060902e+00,\n",
       "           2.25270758e-01,   7.37191660e-01,  -3.84334425e-01,\n",
       "          -1.53574430e-01,  -1.10886631e-01,  -1.06373563e+00,\n",
       "           9.92831729e-01],\n",
       "        [  1.06842594e+00,  -9.69982168e-01,   8.72830429e-01,\n",
       "          -5.82403965e-01,   2.23675866e-01,  -1.20870113e+00,\n",
       "          -4.94871453e-01,   6.41450268e-01,   9.56537352e-01,\n",
       "           1.09752824e+00],\n",
       "        [ -1.03769303e+00,  -8.58420239e-01,  -2.69084477e-01,\n",
       "           4.44790881e-01,   7.70250667e-01,   1.12318122e+00,\n",
       "           7.91582616e-01,  -5.82172799e-01,  -4.22430756e-01,\n",
       "          -5.39928790e-01],\n",
       "        [ -1.04510336e+00,  -4.48871041e-01,   1.12393447e+00,\n",
       "           1.69432807e-01,   1.03164519e+00,  -1.22189591e+00,\n",
       "           2.72666951e-01,  -5.04598656e-01,   4.33685056e-01,\n",
       "          -4.51207483e-01],\n",
       "        [ -1.23870267e+00,  -1.10069228e+00,   1.09699712e+00,\n",
       "          -1.17953950e+00,   1.65962280e-01,   6.02530955e-01,\n",
       "           8.93367727e-01,   5.61241248e-01,   4.66508168e-01,\n",
       "           3.30843476e-01],\n",
       "        [  1.61086473e-01,  -1.58459311e-01,  -1.09802805e+00,\n",
       "           7.27437760e-01,   8.51161027e-01,  -4.14704914e-02,\n",
       "          -3.91743700e-01,   4.29461123e-02,   1.64149796e-02,\n",
       "           3.31953242e-02],\n",
       "        [  1.08722726e+00,   6.33183775e-04,   5.16502065e-01,\n",
       "          -5.12275061e-01,   1.40756667e-01,   8.64594531e-02,\n",
       "           5.60054348e-01,   7.26707167e-01,  -1.23027485e+00,\n",
       "          -6.34411175e-01],\n",
       "        [  6.22196113e-01,  -1.06989631e+00,  -2.43340318e-01,\n",
       "           8.67954540e-01,   1.11644514e+00,   7.04193682e-01,\n",
       "           6.56426320e-01,   2.43307662e-01,  -5.24334678e-01,\n",
       "          -8.00909493e-01],\n",
       "        [ -2.10593092e-01,   3.36292043e-01,  -8.97729458e-01,\n",
       "           1.13276577e+00,  -7.61181593e-01,   7.88386069e-01,\n",
       "           4.38812648e-01,  -9.22103698e-01,  -1.16076113e+00,\n",
       "          -1.31051516e+00],\n",
       "        [ -1.92365139e-01,   6.95548374e-01,   1.83431757e-01,\n",
       "          -8.02156037e-01,  -8.72431027e-01,  -5.10837496e-02,\n",
       "          -1.04787609e+00,  -3.21307897e-01,  -3.18776581e-02,\n",
       "           1.87707863e-02],\n",
       "        [  6.20032706e-01,  -2.97154383e-01,  -1.18899926e+00,\n",
       "          -8.05331874e-01,   7.34282028e-01,   8.69968098e-01,\n",
       "           1.07614663e+00,  -4.52373887e-01,   5.40764859e-01,\n",
       "          -1.25737297e-01],\n",
       "        [  8.06923869e-01,  -2.89504620e-01,   1.43417773e-01,\n",
       "          -7.24978969e-01,  -1.38302901e+00,   1.03387484e-01,\n",
       "           4.94138785e-01,   1.06765127e+00,  -5.29034935e-01,\n",
       "          -1.06510757e-01],\n",
       "        [  1.03113936e+00,   1.12879169e+00,  -1.02545307e-01,\n",
       "           9.88607270e-02,   4.26877162e-01,   1.81160450e-01,\n",
       "          -5.76330305e-01,   4.09469020e-02,  -4.92509426e-01,\n",
       "          -5.30297532e-01],\n",
       "        [  1.19296209e+00,  -1.20389439e+00,  -2.04617830e-01,\n",
       "          -4.11739379e-02,   3.23388083e-01,   1.00674610e+00,\n",
       "          -5.85446388e-01,  -1.21600320e+00,  -1.32076708e-01,\n",
       "          -1.97799674e-01],\n",
       "        [ -3.56002271e-01,  -4.23685647e-01,   2.45757880e-01,\n",
       "           7.54409652e-01,  -7.62774302e-01,  -9.40188762e-01,\n",
       "          -5.13598519e-02,   3.86385732e-01,   1.08634062e-01,\n",
       "          -6.41468998e-02],\n",
       "        [  1.54397598e+00,   3.66425129e-02,  -8.07831065e-01,\n",
       "          -1.01236665e+00,   2.59490471e-01,   1.00154441e-01,\n",
       "          -1.33578621e-01,  -1.00972373e-01,   9.94793929e-01,\n",
       "           6.28254467e-01],\n",
       "        [ -5.60861545e-01,  -6.63395064e-01,   2.22308609e-01,\n",
       "           9.85897034e-01,  -6.62808725e-01,   1.16196116e+00,\n",
       "           6.73985473e-01,  -8.07008788e-01,  -6.98115976e-01,\n",
       "           9.17292672e-01],\n",
       "        [ -4.19307278e-01,   1.31440107e+00,  -8.43839785e-01,\n",
       "           1.15286369e+00,  -1.16453999e+00,   8.35891394e-01,\n",
       "          -3.12229790e-01,   2.38625665e-01,  -7.17959397e-01,\n",
       "           7.20143310e-01],\n",
       "        [  6.20352054e-01,  -1.14300823e+00,   7.18090038e-01,\n",
       "           5.75662947e-01,   1.16627000e+00,  -1.08054871e+00,\n",
       "          -7.81140416e-01,  -1.08943102e+00,   5.71582906e-01,\n",
       "          -1.15496629e+00],\n",
       "        [ -1.12984661e+00,   1.82781317e-01,  -7.51106823e-01,\n",
       "          -2.86823229e-01,  -9.06231525e-01,  -9.21238327e-01,\n",
       "           1.97602361e-01,  -1.13269787e+00,   3.10362255e-01,\n",
       "          -1.02807058e+00],\n",
       "        [ -5.38245574e-01,   2.32470832e-01,  -1.36210090e-02,\n",
       "           3.60606204e-01,  -5.38510144e-01,   1.09328372e+00,\n",
       "           3.14025928e-01,   1.00459872e+00,   8.13918176e-01,\n",
       "          -8.40630618e-02],\n",
       "        [ -1.11057299e+00,   8.29993527e-01,   1.02904474e+00,\n",
       "          -7.27306271e-01,  -7.04612221e-02,   2.26360260e-01,\n",
       "          -9.52586008e-01,   2.49411181e-01,  -1.22798520e+00,\n",
       "          -5.08153291e-01],\n",
       "        [ -9.45924881e-01,   1.43838173e-01,  -9.54863563e-01,\n",
       "          -9.15114366e-02,   3.90670748e-01,  -2.15257320e-01,\n",
       "          -1.25352781e+00,  -1.05614209e+00,   2.57296050e-01,\n",
       "          -7.73810994e-01],\n",
       "        [  1.08612654e+00,  -3.75540584e-01,   8.20208806e-01,\n",
       "          -7.23067930e-01,  -8.35807403e-01,  -1.18468421e+00,\n",
       "          -1.45285897e-01,   2.38167561e-01,   1.11327068e+00,\n",
       "           1.00490804e-01],\n",
       "        [  9.24985211e-02,  -1.19819703e+00,   3.69529198e-01,\n",
       "           2.13823068e-01,   3.07496941e-01,  -3.10419728e-01,\n",
       "           1.17280615e+00,   1.15593370e+00,  -1.20431856e+00,\n",
       "           8.23296048e-01],\n",
       "        [  1.15713114e+00,  -1.06927887e+00,   2.46864040e-01,\n",
       "          -1.04535152e+00,   1.06619622e+00,  -6.12213798e-01,\n",
       "           1.14943999e+00,   2.79196412e-02,   2.69171594e-01,\n",
       "           4.73052647e-01],\n",
       "        [  1.05604280e+00,  -5.07677491e-01,   3.09181225e-01,\n",
       "          -2.85657843e-01,  -1.66082084e-02,   1.22959000e+00,\n",
       "           8.24171263e-01,   8.52304045e-01,  -1.61435183e-01,\n",
       "           9.43388041e-01],\n",
       "        [ -6.33262043e-01,  -7.45855106e-01,  -6.39682619e-01,\n",
       "           6.54416861e-01,  -6.46416434e-01,   2.54721646e-01,\n",
       "          -4.47910080e-01,  -2.78675731e-01,  -2.82705595e-01,\n",
       "           4.27362025e-01],\n",
       "        [ -5.05758621e-01,   2.93245497e-01,  -4.86761234e-01,\n",
       "           1.71232494e-01,  -8.65227750e-01,   3.57730934e-02,\n",
       "          -3.69137254e-01,   4.03206554e-01,  -9.06608267e-01,\n",
       "          -1.22145968e+00],\n",
       "        [ -7.84588092e-01,  -7.75649082e-01,  -8.59269586e-01,\n",
       "          -1.27392038e+00,   5.69641688e-01,  -7.38927912e-01,\n",
       "           9.24941130e-01,   6.21323509e-01,   9.90556297e-01,\n",
       "           3.93785085e-01],\n",
       "        [ -8.74334371e-01,  -6.34257745e-01,   6.29889228e-02,\n",
       "           4.57494422e-01,  -5.04901102e-01,   6.87914483e-01,\n",
       "           1.07474229e-01,  -4.82390593e-01,  -1.25293825e+00,\n",
       "           1.90561016e-02],\n",
       "        [ -4.74232149e-01,  -1.00568378e+00,  -1.15380356e+00,\n",
       "           1.04846267e+00,  -6.99115752e-01,  -1.49450059e-01,\n",
       "          -9.93393312e-01,  -8.29279921e-01,   5.81454757e-02,\n",
       "          -5.73717306e-01],\n",
       "        [  2.57206527e-01,  -2.75679018e-01,  -8.87302389e-01,\n",
       "          -1.33463165e+00,  -4.75337067e-01,   5.55249657e-01,\n",
       "          -1.22471499e+00,   1.01282478e+00,   9.46713676e-01,\n",
       "           4.59710968e-01],\n",
       "        [ -4.08628270e-02,  -4.75125825e-01,   1.02936192e+00,\n",
       "          -9.94493010e-01,   8.33855516e-01,   7.19879202e-01,\n",
       "           9.47978768e-01,  -5.45172376e-01,   4.71632717e-01,\n",
       "          -1.01717345e+00],\n",
       "        [  3.76260037e-01,  -1.15674675e+00,  -3.88573710e-01,\n",
       "           1.20053043e+00,  -4.06832331e-01,  -6.80076838e-01,\n",
       "          -1.50921199e-01,  -1.07616012e+00,  -9.98665006e-01,\n",
       "           9.43416442e-01],\n",
       "        [  1.20137018e+00,  -3.62704255e-02,  -1.20496444e+00,\n",
       "           1.20684786e+00,  -7.71215096e-01,  -1.29193600e-01,\n",
       "          -4.38329909e-01,  -9.08300040e-02,  -5.93133997e-01,\n",
       "          -4.01254337e-01],\n",
       "        [ -4.94614614e-01,  -1.30826240e+00,  -9.52029616e-01,\n",
       "          -1.18870256e+00,   2.30379185e-01,  -1.00877213e-01,\n",
       "          -9.96539206e-01,  -6.81902360e-01,  -9.13047045e-01,\n",
       "           1.27398213e-01],\n",
       "        [ -4.72294012e-01,  -2.02411021e-01,   1.22857233e+00,\n",
       "           4.43145537e-02,   6.97344247e-02,  -7.68069924e-01,\n",
       "          -6.20370654e-01,  -1.97878171e-01,   1.70429136e-01,\n",
       "           9.25372482e-01],\n",
       "        [  8.79394310e-01,  -7.87836180e-01,   4.09280788e-01,\n",
       "           6.29872855e-01,  -5.12622077e-01,   9.29825450e-01,\n",
       "          -1.10395240e-01,  -3.39357697e-01,  -1.75421263e-01,\n",
       "          -7.28892641e-01],\n",
       "        [ -3.76846674e-01,   1.35774872e+00,  -2.07911112e-01,\n",
       "          -6.81061047e-02,   1.76703382e-01,  -7.59250908e-01,\n",
       "          -4.05462815e-01,   8.92320834e-01,  -1.58256430e-01,\n",
       "           1.07954043e+00],\n",
       "        [  9.34807700e-01,   7.37075117e-01,   5.26908582e-02,\n",
       "           4.34002102e-01,  -4.37676975e-01,   9.17760389e-01,\n",
       "           2.66840587e-01,   2.20242714e-01,  -7.99577963e-01,\n",
       "           5.54058876e-01],\n",
       "        [  9.24114455e-03,   7.88506096e-01,   1.25764397e+00,\n",
       "          -1.14200123e+00,   1.15508877e+00,   4.39527643e-01,\n",
       "           1.79407083e-01,  -4.72793034e-01,   8.11309284e-01,\n",
       "           1.24018917e-01],\n",
       "        [  1.01057138e+00,   7.25737482e-01,  -2.80948776e-01,\n",
       "           6.43882432e-01,  -4.95187286e-01,   5.00700210e-01,\n",
       "          -2.60834519e-01,  -5.56089366e-01,   9.11911657e-01,\n",
       "          -2.66495259e-01],\n",
       "        [ -6.22644165e-01,   1.00178950e+00,  -1.01215028e+00,\n",
       "           3.70886779e-01,   1.41181059e-01,   5.27653666e-01,\n",
       "           1.78136379e-01,  -8.03173783e-01,  -7.80579036e-02,\n",
       "          -4.16616084e-01],\n",
       "        [  5.20904680e-01,   1.32228312e-01,  -3.02064307e-01,\n",
       "           1.58327657e-01,  -7.06654442e-01,  -7.11132026e-01,\n",
       "          -1.00612907e-02,   1.09878875e+00,  -1.08786198e+00,\n",
       "          -1.14022199e-01]])}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bornacnn.mlp_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abs'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x='ABS'\n",
    "x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
